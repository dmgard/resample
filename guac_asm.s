// Code generated by command: go run main.go -out guac_asm.s -stubs guac_stubs.go. DO NOT EDIT.

#include "textflag.h"

// func ResampleFixedF32_8x2(Out []float32, In []float32, Coefs []float32, CoefIdx int, OutIdx int, OutStep int) (CoefIdxOut int, OutIdxOut int)
// Requires: AVX, CMOV, FMA3
TEXT ·ResampleFixedF32_8x2(SB), NOSPLIT, $0-112
	MOVQ Out_base+0(FP), AX
	MOVQ In_base+24(FP), CX
	MOVQ Coefs_base+48(FP), DX
	MOVQ CoefIdx+72(FP), BX
	MOVQ Coefs_len+56(FP), SI
	MOVQ OutIdx+80(FP), DI
	MOVQ Out_len+8(FP), R8
	SUBQ $+1, R8
	MOVQ OutStep+88(FP), R9

	// Reload previous partially accumulated output samples
	// Compute output vector index as 
	// (fixedPointIndex / fixedPointScale / vectorLength * vectorLength) % outBufferLength
	// wraps within the output buffer and quantizes the nearest vector register multiple
	MOVQ DI, R10
	SHRQ $+35, R10
	SHLQ $+3, R10
	ANDQ R8, R10

	// Re-load partially accumulated output samples, wrapping ringbuffer
	MOVQ    R10, R11
	VMOVUPS (AX)(R11*4), Y0
	ADDQ    $+8, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Y1

	// For each input sample:
	XORQ R11, R11
	MOVQ In_len+32(FP), R12
	SUBQ $0x00000001, R12

In0:
	CMPQ R12, R11
	JL   In0end

	// The coefficient load index is (filtertaps+padding) * wrappedInputIndex
	// + (outAlignedIdx*vectorLength - outSampleIdx
	// This loads each coefficient set within a block with proper zero padding
	// so that multiple coefficient sets can be accumulated into one set of registers,
	// offset by the proper number of in-register samples
	// Broadcast the current input sample and contribute and accumulate its output-phase-specific-coefficient-scaled individual contribution to every output sample in range
	VBROADCASTSS (CX)(R11*4), Y2
	VFMADD231PS  (DX)(BX*4), Y2, Y0
	VFMADD231PS  32(DX)(BX*4), Y2, Y1

	// If incrementing the output index crosses a multiple of vectorLength,
	// the lowest register is completely accumulated and can be stored while
	// the rest are shifted down in its place
	// Compute current register index
	MOVQ DI, R13
	SHRQ $+35, R13

	// Increment current output sample index and compute next register index
	ADDQ R9, DI
	MOVQ DI, R14
	SHRQ $+35, R14

	// Store and shift registers if a register transition has happened
	CMPQ    R13, R14
	JE      no_store
	VMOVUPS Y0, (AX)(R10*4)
	VMOVUPS Y1, Y0
	VXORPS  Y1, Y1, Y1

no_store:
	// Update and wrap the vector-aligned output index
	MOVQ R14, R10
	SHLQ $+3, R10
	ANDQ R8, R10

	// Update and wrap coefficient index
	XORQ R13, R13
	ADDQ $+32, BX
	CMPQ BX, SI

	// Wrap phase counter - SUB changes flags so do this after to avoid clobbering Compare result
	CMOVQGE SI, R13
	SUBQ    R13, BX
	ADDQ    $0x00000001, R11
	JMP     In0

In0end:
	// Store each partially accumulated vector to the output slice
	// taking care to wrap into output ringbuffer
	VMOVUPS Y0, (AX)(R10*4)
	ADDQ    $+8, R10
	ANDQ    R8, R10
	VMOVUPS Y1, (AX)(R10*4)
	VZEROUPPER

	// Return the latest phase and output index for reuse in future calls
	MOVQ BX, CoefIdxOut+96(FP)
	MOVQ DI, OutIdxOut+104(FP)
	RET

// func ResampleFixedF32_8x3(Out []float32, In []float32, Coefs []float32, CoefIdx int, OutIdx int, OutStep int) (CoefIdxOut int, OutIdxOut int)
// Requires: AVX, CMOV, FMA3
TEXT ·ResampleFixedF32_8x3(SB), NOSPLIT, $0-112
	MOVQ Out_base+0(FP), AX
	MOVQ In_base+24(FP), CX
	MOVQ Coefs_base+48(FP), DX
	MOVQ CoefIdx+72(FP), BX
	MOVQ Coefs_len+56(FP), SI
	MOVQ OutIdx+80(FP), DI
	MOVQ Out_len+8(FP), R8
	SUBQ $+1, R8
	MOVQ OutStep+88(FP), R9

	// Reload previous partially accumulated output samples
	// Compute output vector index as 
	// (fixedPointIndex / fixedPointScale / vectorLength * vectorLength) % outBufferLength
	// wraps within the output buffer and quantizes the nearest vector register multiple
	MOVQ DI, R10
	SHRQ $+35, R10
	SHLQ $+3, R10
	ANDQ R8, R10

	// Re-load partially accumulated output samples, wrapping ringbuffer
	MOVQ    R10, R11
	VMOVUPS (AX)(R11*4), Y0
	ADDQ    $+8, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Y1
	ADDQ    $+8, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Y2

	// For each input sample:
	XORQ R11, R11
	MOVQ In_len+32(FP), R12
	SUBQ $0x00000001, R12

In0:
	CMPQ R12, R11
	JL   In0end

	// The coefficient load index is (filtertaps+padding) * wrappedInputIndex
	// + (outAlignedIdx*vectorLength - outSampleIdx
	// This loads each coefficient set within a block with proper zero padding
	// so that multiple coefficient sets can be accumulated into one set of registers,
	// offset by the proper number of in-register samples
	// Broadcast the current input sample and contribute and accumulate its output-phase-specific-coefficient-scaled individual contribution to every output sample in range
	VBROADCASTSS (CX)(R11*4), Y3
	VFMADD231PS  (DX)(BX*4), Y3, Y0
	VFMADD231PS  32(DX)(BX*4), Y3, Y1
	VFMADD231PS  64(DX)(BX*4), Y3, Y2

	// If incrementing the output index crosses a multiple of vectorLength,
	// the lowest register is completely accumulated and can be stored while
	// the rest are shifted down in its place
	// Compute current register index
	MOVQ DI, R13
	SHRQ $+35, R13

	// Increment current output sample index and compute next register index
	ADDQ R9, DI
	MOVQ DI, R14
	SHRQ $+35, R14

	// Store and shift registers if a register transition has happened
	CMPQ    R13, R14
	JE      no_store
	VMOVUPS Y0, (AX)(R10*4)
	VMOVUPS Y1, Y0
	VMOVUPS Y2, Y1
	VXORPS  Y2, Y2, Y2

no_store:
	// Update and wrap the vector-aligned output index
	MOVQ R14, R10
	SHLQ $+3, R10
	ANDQ R8, R10

	// Update and wrap coefficient index
	XORQ R13, R13
	ADDQ $+40, BX
	CMPQ BX, SI

	// Wrap phase counter - SUB changes flags so do this after to avoid clobbering Compare result
	CMOVQGE SI, R13
	SUBQ    R13, BX
	ADDQ    $0x00000001, R11
	JMP     In0

In0end:
	// Store each partially accumulated vector to the output slice
	// taking care to wrap into output ringbuffer
	VMOVUPS Y0, (AX)(R10*4)
	ADDQ    $+8, R10
	ANDQ    R8, R10
	VMOVUPS Y1, (AX)(R10*4)
	ADDQ    $+8, R10
	ANDQ    R8, R10
	VMOVUPS Y2, (AX)(R10*4)
	VZEROUPPER

	// Return the latest phase and output index for reuse in future calls
	MOVQ BX, CoefIdxOut+96(FP)
	MOVQ DI, OutIdxOut+104(FP)
	RET

// func ResampleFixedF32_8x4(Out []float32, In []float32, Coefs []float32, CoefIdx int, OutIdx int, OutStep int) (CoefIdxOut int, OutIdxOut int)
// Requires: AVX, CMOV, FMA3
TEXT ·ResampleFixedF32_8x4(SB), NOSPLIT, $0-112
	MOVQ Out_base+0(FP), AX
	MOVQ In_base+24(FP), CX
	MOVQ Coefs_base+48(FP), DX
	MOVQ CoefIdx+72(FP), BX
	MOVQ Coefs_len+56(FP), SI
	MOVQ OutIdx+80(FP), DI
	MOVQ Out_len+8(FP), R8
	SUBQ $+1, R8
	MOVQ OutStep+88(FP), R9

	// Reload previous partially accumulated output samples
	// Compute output vector index as 
	// (fixedPointIndex / fixedPointScale / vectorLength * vectorLength) % outBufferLength
	// wraps within the output buffer and quantizes the nearest vector register multiple
	MOVQ DI, R10
	SHRQ $+35, R10
	SHLQ $+3, R10
	ANDQ R8, R10

	// Re-load partially accumulated output samples, wrapping ringbuffer
	MOVQ    R10, R11
	VMOVUPS (AX)(R11*4), Y0
	ADDQ    $+8, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Y1
	ADDQ    $+8, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Y2
	ADDQ    $+8, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Y3

	// For each input sample:
	XORQ R11, R11
	MOVQ In_len+32(FP), R12
	SUBQ $0x00000001, R12

In0:
	CMPQ R12, R11
	JL   In0end

	// The coefficient load index is (filtertaps+padding) * wrappedInputIndex
	// + (outAlignedIdx*vectorLength - outSampleIdx
	// This loads each coefficient set within a block with proper zero padding
	// so that multiple coefficient sets can be accumulated into one set of registers,
	// offset by the proper number of in-register samples
	// Broadcast the current input sample and contribute and accumulate its output-phase-specific-coefficient-scaled individual contribution to every output sample in range
	VBROADCASTSS (CX)(R11*4), Y4
	VFMADD231PS  (DX)(BX*4), Y4, Y0
	VFMADD231PS  32(DX)(BX*4), Y4, Y1
	VFMADD231PS  64(DX)(BX*4), Y4, Y2
	VFMADD231PS  96(DX)(BX*4), Y4, Y3

	// If incrementing the output index crosses a multiple of vectorLength,
	// the lowest register is completely accumulated and can be stored while
	// the rest are shifted down in its place
	// Compute current register index
	MOVQ DI, R13
	SHRQ $+35, R13

	// Increment current output sample index and compute next register index
	ADDQ R9, DI
	MOVQ DI, R14
	SHRQ $+35, R14

	// Store and shift registers if a register transition has happened
	CMPQ    R13, R14
	JE      no_store
	VMOVUPS Y0, (AX)(R10*4)
	VMOVUPS Y1, Y0
	VMOVUPS Y2, Y1
	VMOVUPS Y3, Y2
	VXORPS  Y3, Y3, Y3

no_store:
	// Update and wrap the vector-aligned output index
	MOVQ R14, R10
	SHLQ $+3, R10
	ANDQ R8, R10

	// Update and wrap coefficient index
	XORQ R13, R13
	ADDQ $+48, BX
	CMPQ BX, SI

	// Wrap phase counter - SUB changes flags so do this after to avoid clobbering Compare result
	CMOVQGE SI, R13
	SUBQ    R13, BX
	ADDQ    $0x00000001, R11
	JMP     In0

In0end:
	// Store each partially accumulated vector to the output slice
	// taking care to wrap into output ringbuffer
	VMOVUPS Y0, (AX)(R10*4)
	ADDQ    $+8, R10
	ANDQ    R8, R10
	VMOVUPS Y1, (AX)(R10*4)
	ADDQ    $+8, R10
	ANDQ    R8, R10
	VMOVUPS Y2, (AX)(R10*4)
	ADDQ    $+8, R10
	ANDQ    R8, R10
	VMOVUPS Y3, (AX)(R10*4)
	VZEROUPPER

	// Return the latest phase and output index for reuse in future calls
	MOVQ BX, CoefIdxOut+96(FP)
	MOVQ DI, OutIdxOut+104(FP)
	RET

// func ResampleFixedF32_8x5(Out []float32, In []float32, Coefs []float32, CoefIdx int, OutIdx int, OutStep int) (CoefIdxOut int, OutIdxOut int)
// Requires: AVX, CMOV, FMA3
TEXT ·ResampleFixedF32_8x5(SB), NOSPLIT, $0-112
	MOVQ Out_base+0(FP), AX
	MOVQ In_base+24(FP), CX
	MOVQ Coefs_base+48(FP), DX
	MOVQ CoefIdx+72(FP), BX
	MOVQ Coefs_len+56(FP), SI
	MOVQ OutIdx+80(FP), DI
	MOVQ Out_len+8(FP), R8
	SUBQ $+1, R8
	MOVQ OutStep+88(FP), R9

	// Reload previous partially accumulated output samples
	// Compute output vector index as 
	// (fixedPointIndex / fixedPointScale / vectorLength * vectorLength) % outBufferLength
	// wraps within the output buffer and quantizes the nearest vector register multiple
	MOVQ DI, R10
	SHRQ $+35, R10
	SHLQ $+3, R10
	ANDQ R8, R10

	// Re-load partially accumulated output samples, wrapping ringbuffer
	MOVQ    R10, R11
	VMOVUPS (AX)(R11*4), Y0
	ADDQ    $+8, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Y1
	ADDQ    $+8, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Y2
	ADDQ    $+8, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Y3
	ADDQ    $+8, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Y4

	// For each input sample:
	XORQ R11, R11
	MOVQ In_len+32(FP), R12
	SUBQ $0x00000001, R12

In0:
	CMPQ R12, R11
	JL   In0end

	// The coefficient load index is (filtertaps+padding) * wrappedInputIndex
	// + (outAlignedIdx*vectorLength - outSampleIdx
	// This loads each coefficient set within a block with proper zero padding
	// so that multiple coefficient sets can be accumulated into one set of registers,
	// offset by the proper number of in-register samples
	// Broadcast the current input sample and contribute and accumulate its output-phase-specific-coefficient-scaled individual contribution to every output sample in range
	VBROADCASTSS (CX)(R11*4), Y5
	VFMADD231PS  (DX)(BX*4), Y5, Y0
	VFMADD231PS  32(DX)(BX*4), Y5, Y1
	VFMADD231PS  64(DX)(BX*4), Y5, Y2
	VFMADD231PS  96(DX)(BX*4), Y5, Y3
	VFMADD231PS  128(DX)(BX*4), Y5, Y4

	// If incrementing the output index crosses a multiple of vectorLength,
	// the lowest register is completely accumulated and can be stored while
	// the rest are shifted down in its place
	// Compute current register index
	MOVQ DI, R13
	SHRQ $+35, R13

	// Increment current output sample index and compute next register index
	ADDQ R9, DI
	MOVQ DI, R14
	SHRQ $+35, R14

	// Store and shift registers if a register transition has happened
	CMPQ    R13, R14
	JE      no_store
	VMOVUPS Y0, (AX)(R10*4)
	VMOVUPS Y1, Y0
	VMOVUPS Y2, Y1
	VMOVUPS Y3, Y2
	VMOVUPS Y4, Y3
	VXORPS  Y4, Y4, Y4

no_store:
	// Update and wrap the vector-aligned output index
	MOVQ R14, R10
	SHLQ $+3, R10
	ANDQ R8, R10

	// Update and wrap coefficient index
	XORQ R13, R13
	ADDQ $+56, BX
	CMPQ BX, SI

	// Wrap phase counter - SUB changes flags so do this after to avoid clobbering Compare result
	CMOVQGE SI, R13
	SUBQ    R13, BX
	ADDQ    $0x00000001, R11
	JMP     In0

In0end:
	// Store each partially accumulated vector to the output slice
	// taking care to wrap into output ringbuffer
	VMOVUPS Y0, (AX)(R10*4)
	ADDQ    $+8, R10
	ANDQ    R8, R10
	VMOVUPS Y1, (AX)(R10*4)
	ADDQ    $+8, R10
	ANDQ    R8, R10
	VMOVUPS Y2, (AX)(R10*4)
	ADDQ    $+8, R10
	ANDQ    R8, R10
	VMOVUPS Y3, (AX)(R10*4)
	ADDQ    $+8, R10
	ANDQ    R8, R10
	VMOVUPS Y4, (AX)(R10*4)
	VZEROUPPER

	// Return the latest phase and output index for reuse in future calls
	MOVQ BX, CoefIdxOut+96(FP)
	MOVQ DI, OutIdxOut+104(FP)
	RET

// func ResampleFixedF32_8x6(Out []float32, In []float32, Coefs []float32, CoefIdx int, OutIdx int, OutStep int) (CoefIdxOut int, OutIdxOut int)
// Requires: AVX, CMOV, FMA3
TEXT ·ResampleFixedF32_8x6(SB), NOSPLIT, $0-112
	MOVQ Out_base+0(FP), AX
	MOVQ In_base+24(FP), CX
	MOVQ Coefs_base+48(FP), DX
	MOVQ CoefIdx+72(FP), BX
	MOVQ Coefs_len+56(FP), SI
	MOVQ OutIdx+80(FP), DI
	MOVQ Out_len+8(FP), R8
	SUBQ $+1, R8
	MOVQ OutStep+88(FP), R9

	// Reload previous partially accumulated output samples
	// Compute output vector index as 
	// (fixedPointIndex / fixedPointScale / vectorLength * vectorLength) % outBufferLength
	// wraps within the output buffer and quantizes the nearest vector register multiple
	MOVQ DI, R10
	SHRQ $+35, R10
	SHLQ $+3, R10
	ANDQ R8, R10

	// Re-load partially accumulated output samples, wrapping ringbuffer
	MOVQ    R10, R11
	VMOVUPS (AX)(R11*4), Y0
	ADDQ    $+8, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Y1
	ADDQ    $+8, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Y2
	ADDQ    $+8, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Y3
	ADDQ    $+8, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Y4
	ADDQ    $+8, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Y5

	// For each input sample:
	XORQ R11, R11
	MOVQ In_len+32(FP), R12
	SUBQ $0x00000001, R12

In0:
	CMPQ R12, R11
	JL   In0end

	// The coefficient load index is (filtertaps+padding) * wrappedInputIndex
	// + (outAlignedIdx*vectorLength - outSampleIdx
	// This loads each coefficient set within a block with proper zero padding
	// so that multiple coefficient sets can be accumulated into one set of registers,
	// offset by the proper number of in-register samples
	// Broadcast the current input sample and contribute and accumulate its output-phase-specific-coefficient-scaled individual contribution to every output sample in range
	VBROADCASTSS (CX)(R11*4), Y6
	VFMADD231PS  (DX)(BX*4), Y6, Y0
	VFMADD231PS  32(DX)(BX*4), Y6, Y1
	VFMADD231PS  64(DX)(BX*4), Y6, Y2
	VFMADD231PS  96(DX)(BX*4), Y6, Y3
	VFMADD231PS  128(DX)(BX*4), Y6, Y4
	VFMADD231PS  160(DX)(BX*4), Y6, Y5

	// If incrementing the output index crosses a multiple of vectorLength,
	// the lowest register is completely accumulated and can be stored while
	// the rest are shifted down in its place
	// Compute current register index
	MOVQ DI, R13
	SHRQ $+35, R13

	// Increment current output sample index and compute next register index
	ADDQ R9, DI
	MOVQ DI, R14
	SHRQ $+35, R14

	// Store and shift registers if a register transition has happened
	CMPQ    R13, R14
	JE      no_store
	VMOVUPS Y0, (AX)(R10*4)
	VMOVUPS Y1, Y0
	VMOVUPS Y2, Y1
	VMOVUPS Y3, Y2
	VMOVUPS Y4, Y3
	VMOVUPS Y5, Y4
	VXORPS  Y5, Y5, Y5

no_store:
	// Update and wrap the vector-aligned output index
	MOVQ R14, R10
	SHLQ $+3, R10
	ANDQ R8, R10

	// Update and wrap coefficient index
	XORQ R13, R13
	ADDQ $+64, BX
	CMPQ BX, SI

	// Wrap phase counter - SUB changes flags so do this after to avoid clobbering Compare result
	CMOVQGE SI, R13
	SUBQ    R13, BX
	ADDQ    $0x00000001, R11
	JMP     In0

In0end:
	// Store each partially accumulated vector to the output slice
	// taking care to wrap into output ringbuffer
	VMOVUPS Y0, (AX)(R10*4)
	ADDQ    $+8, R10
	ANDQ    R8, R10
	VMOVUPS Y1, (AX)(R10*4)
	ADDQ    $+8, R10
	ANDQ    R8, R10
	VMOVUPS Y2, (AX)(R10*4)
	ADDQ    $+8, R10
	ANDQ    R8, R10
	VMOVUPS Y3, (AX)(R10*4)
	ADDQ    $+8, R10
	ANDQ    R8, R10
	VMOVUPS Y4, (AX)(R10*4)
	ADDQ    $+8, R10
	ANDQ    R8, R10
	VMOVUPS Y5, (AX)(R10*4)
	VZEROUPPER

	// Return the latest phase and output index for reuse in future calls
	MOVQ BX, CoefIdxOut+96(FP)
	MOVQ DI, OutIdxOut+104(FP)
	RET

// func ResampleFixedF32_8x7(Out []float32, In []float32, Coefs []float32, CoefIdx int, OutIdx int, OutStep int) (CoefIdxOut int, OutIdxOut int)
// Requires: AVX, CMOV, FMA3
TEXT ·ResampleFixedF32_8x7(SB), NOSPLIT, $0-112
	MOVQ Out_base+0(FP), AX
	MOVQ In_base+24(FP), CX
	MOVQ Coefs_base+48(FP), DX
	MOVQ CoefIdx+72(FP), BX
	MOVQ Coefs_len+56(FP), SI
	MOVQ OutIdx+80(FP), DI
	MOVQ Out_len+8(FP), R8
	SUBQ $+1, R8
	MOVQ OutStep+88(FP), R9

	// Reload previous partially accumulated output samples
	// Compute output vector index as 
	// (fixedPointIndex / fixedPointScale / vectorLength * vectorLength) % outBufferLength
	// wraps within the output buffer and quantizes the nearest vector register multiple
	MOVQ DI, R10
	SHRQ $+35, R10
	SHLQ $+3, R10
	ANDQ R8, R10

	// Re-load partially accumulated output samples, wrapping ringbuffer
	MOVQ    R10, R11
	VMOVUPS (AX)(R11*4), Y0
	ADDQ    $+8, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Y1
	ADDQ    $+8, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Y2
	ADDQ    $+8, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Y3
	ADDQ    $+8, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Y4
	ADDQ    $+8, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Y5
	ADDQ    $+8, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Y6

	// For each input sample:
	XORQ R11, R11
	MOVQ In_len+32(FP), R12
	SUBQ $0x00000001, R12

In0:
	CMPQ R12, R11
	JL   In0end

	// The coefficient load index is (filtertaps+padding) * wrappedInputIndex
	// + (outAlignedIdx*vectorLength - outSampleIdx
	// This loads each coefficient set within a block with proper zero padding
	// so that multiple coefficient sets can be accumulated into one set of registers,
	// offset by the proper number of in-register samples
	// Broadcast the current input sample and contribute and accumulate its output-phase-specific-coefficient-scaled individual contribution to every output sample in range
	VBROADCASTSS (CX)(R11*4), Y7
	VFMADD231PS  (DX)(BX*4), Y7, Y0
	VFMADD231PS  32(DX)(BX*4), Y7, Y1
	VFMADD231PS  64(DX)(BX*4), Y7, Y2
	VFMADD231PS  96(DX)(BX*4), Y7, Y3
	VFMADD231PS  128(DX)(BX*4), Y7, Y4
	VFMADD231PS  160(DX)(BX*4), Y7, Y5
	VFMADD231PS  192(DX)(BX*4), Y7, Y6

	// If incrementing the output index crosses a multiple of vectorLength,
	// the lowest register is completely accumulated and can be stored while
	// the rest are shifted down in its place
	// Compute current register index
	MOVQ DI, R13
	SHRQ $+35, R13

	// Increment current output sample index and compute next register index
	ADDQ R9, DI
	MOVQ DI, R14
	SHRQ $+35, R14

	// Store and shift registers if a register transition has happened
	CMPQ    R13, R14
	JE      no_store
	VMOVUPS Y0, (AX)(R10*4)
	VMOVUPS Y1, Y0
	VMOVUPS Y2, Y1
	VMOVUPS Y3, Y2
	VMOVUPS Y4, Y3
	VMOVUPS Y5, Y4
	VMOVUPS Y6, Y5
	VXORPS  Y6, Y6, Y6

no_store:
	// Update and wrap the vector-aligned output index
	MOVQ R14, R10
	SHLQ $+3, R10
	ANDQ R8, R10

	// Update and wrap coefficient index
	XORQ R13, R13
	ADDQ $+72, BX
	CMPQ BX, SI

	// Wrap phase counter - SUB changes flags so do this after to avoid clobbering Compare result
	CMOVQGE SI, R13
	SUBQ    R13, BX
	ADDQ    $0x00000001, R11
	JMP     In0

In0end:
	// Store each partially accumulated vector to the output slice
	// taking care to wrap into output ringbuffer
	VMOVUPS Y0, (AX)(R10*4)
	ADDQ    $+8, R10
	ANDQ    R8, R10
	VMOVUPS Y1, (AX)(R10*4)
	ADDQ    $+8, R10
	ANDQ    R8, R10
	VMOVUPS Y2, (AX)(R10*4)
	ADDQ    $+8, R10
	ANDQ    R8, R10
	VMOVUPS Y3, (AX)(R10*4)
	ADDQ    $+8, R10
	ANDQ    R8, R10
	VMOVUPS Y4, (AX)(R10*4)
	ADDQ    $+8, R10
	ANDQ    R8, R10
	VMOVUPS Y5, (AX)(R10*4)
	ADDQ    $+8, R10
	ANDQ    R8, R10
	VMOVUPS Y6, (AX)(R10*4)
	VZEROUPPER

	// Return the latest phase and output index for reuse in future calls
	MOVQ BX, CoefIdxOut+96(FP)
	MOVQ DI, OutIdxOut+104(FP)
	RET

// func ResampleFixedF32_8x8(Out []float32, In []float32, Coefs []float32, CoefIdx int, OutIdx int, OutStep int) (CoefIdxOut int, OutIdxOut int)
// Requires: AVX, CMOV, FMA3
TEXT ·ResampleFixedF32_8x8(SB), NOSPLIT, $0-112
	MOVQ Out_base+0(FP), AX
	MOVQ In_base+24(FP), CX
	MOVQ Coefs_base+48(FP), DX
	MOVQ CoefIdx+72(FP), BX
	MOVQ Coefs_len+56(FP), SI
	MOVQ OutIdx+80(FP), DI
	MOVQ Out_len+8(FP), R8
	SUBQ $+1, R8
	MOVQ OutStep+88(FP), R9

	// Reload previous partially accumulated output samples
	// Compute output vector index as 
	// (fixedPointIndex / fixedPointScale / vectorLength * vectorLength) % outBufferLength
	// wraps within the output buffer and quantizes the nearest vector register multiple
	MOVQ DI, R10
	SHRQ $+35, R10
	SHLQ $+3, R10
	ANDQ R8, R10

	// Re-load partially accumulated output samples, wrapping ringbuffer
	MOVQ    R10, R11
	VMOVUPS (AX)(R11*4), Y0
	ADDQ    $+8, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Y1
	ADDQ    $+8, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Y2
	ADDQ    $+8, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Y3
	ADDQ    $+8, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Y4
	ADDQ    $+8, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Y5
	ADDQ    $+8, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Y6
	ADDQ    $+8, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Y7

	// For each input sample:
	XORQ R11, R11
	MOVQ In_len+32(FP), R12
	SUBQ $0x00000001, R12

In0:
	CMPQ R12, R11
	JL   In0end

	// The coefficient load index is (filtertaps+padding) * wrappedInputIndex
	// + (outAlignedIdx*vectorLength - outSampleIdx
	// This loads each coefficient set within a block with proper zero padding
	// so that multiple coefficient sets can be accumulated into one set of registers,
	// offset by the proper number of in-register samples
	// Broadcast the current input sample and contribute and accumulate its output-phase-specific-coefficient-scaled individual contribution to every output sample in range
	VBROADCASTSS (CX)(R11*4), Y8
	VFMADD231PS  (DX)(BX*4), Y8, Y0
	VFMADD231PS  32(DX)(BX*4), Y8, Y1
	VFMADD231PS  64(DX)(BX*4), Y8, Y2
	VFMADD231PS  96(DX)(BX*4), Y8, Y3
	VFMADD231PS  128(DX)(BX*4), Y8, Y4
	VFMADD231PS  160(DX)(BX*4), Y8, Y5
	VFMADD231PS  192(DX)(BX*4), Y8, Y6
	VFMADD231PS  224(DX)(BX*4), Y8, Y7

	// If incrementing the output index crosses a multiple of vectorLength,
	// the lowest register is completely accumulated and can be stored while
	// the rest are shifted down in its place
	// Compute current register index
	MOVQ DI, R13
	SHRQ $+35, R13

	// Increment current output sample index and compute next register index
	ADDQ R9, DI
	MOVQ DI, R14
	SHRQ $+35, R14

	// Store and shift registers if a register transition has happened
	CMPQ    R13, R14
	JE      no_store
	VMOVUPS Y0, (AX)(R10*4)
	VMOVUPS Y1, Y0
	VMOVUPS Y2, Y1
	VMOVUPS Y3, Y2
	VMOVUPS Y4, Y3
	VMOVUPS Y5, Y4
	VMOVUPS Y6, Y5
	VMOVUPS Y7, Y6
	VXORPS  Y7, Y7, Y7

no_store:
	// Update and wrap the vector-aligned output index
	MOVQ R14, R10
	SHLQ $+3, R10
	ANDQ R8, R10

	// Update and wrap coefficient index
	XORQ R13, R13
	ADDQ $+80, BX
	CMPQ BX, SI

	// Wrap phase counter - SUB changes flags so do this after to avoid clobbering Compare result
	CMOVQGE SI, R13
	SUBQ    R13, BX
	ADDQ    $0x00000001, R11
	JMP     In0

In0end:
	// Store each partially accumulated vector to the output slice
	// taking care to wrap into output ringbuffer
	VMOVUPS Y0, (AX)(R10*4)
	ADDQ    $+8, R10
	ANDQ    R8, R10
	VMOVUPS Y1, (AX)(R10*4)
	ADDQ    $+8, R10
	ANDQ    R8, R10
	VMOVUPS Y2, (AX)(R10*4)
	ADDQ    $+8, R10
	ANDQ    R8, R10
	VMOVUPS Y3, (AX)(R10*4)
	ADDQ    $+8, R10
	ANDQ    R8, R10
	VMOVUPS Y4, (AX)(R10*4)
	ADDQ    $+8, R10
	ANDQ    R8, R10
	VMOVUPS Y5, (AX)(R10*4)
	ADDQ    $+8, R10
	ANDQ    R8, R10
	VMOVUPS Y6, (AX)(R10*4)
	ADDQ    $+8, R10
	ANDQ    R8, R10
	VMOVUPS Y7, (AX)(R10*4)
	VZEROUPPER

	// Return the latest phase and output index for reuse in future calls
	MOVQ BX, CoefIdxOut+96(FP)
	MOVQ DI, OutIdxOut+104(FP)
	RET

// func ResampleFixedF32_16x2(Out []float32, In []float32, Coefs []float32, CoefIdx int, OutIdx int, OutStep int) (CoefIdxOut int, OutIdxOut int)
// Requires: AVX, AVX512DQ, AVX512F, CMOV
TEXT ·ResampleFixedF32_16x2(SB), NOSPLIT, $0-112
	MOVQ Out_base+0(FP), AX
	MOVQ In_base+24(FP), CX
	MOVQ Coefs_base+48(FP), DX
	MOVQ CoefIdx+72(FP), BX
	MOVQ Coefs_len+56(FP), SI
	MOVQ OutIdx+80(FP), DI
	MOVQ Out_len+8(FP), R8
	SUBQ $+1, R8
	MOVQ OutStep+88(FP), R9

	// Reload previous partially accumulated output samples
	// Compute output vector index as 
	// (fixedPointIndex / fixedPointScale / vectorLength * vectorLength) % outBufferLength
	// wraps within the output buffer and quantizes the nearest vector register multiple
	MOVQ DI, R10
	SHRQ $+36, R10
	SHLQ $+4, R10
	ANDQ R8, R10

	// Re-load partially accumulated output samples, wrapping ringbuffer
	MOVQ    R10, R11
	VMOVUPS (AX)(R11*4), Z0
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z1

	// For each input sample:
	XORQ R11, R11
	MOVQ In_len+32(FP), R12
	SUBQ $0x00000001, R12

In0:
	CMPQ R12, R11
	JL   In0end

	// The coefficient load index is (filtertaps+padding) * wrappedInputIndex
	// + (outAlignedIdx*vectorLength - outSampleIdx
	// This loads each coefficient set within a block with proper zero padding
	// so that multiple coefficient sets can be accumulated into one set of registers,
	// offset by the proper number of in-register samples
	// Broadcast the current input sample and contribute and accumulate its output-phase-specific-coefficient-scaled individual contribution to every output sample in range
	VBROADCASTSS (CX)(R11*4), Z2
	VFMADD231PS  (DX)(BX*4), Z2, Z0
	VFMADD231PS  64(DX)(BX*4), Z2, Z1

	// If incrementing the output index crosses a multiple of vectorLength,
	// the lowest register is completely accumulated and can be stored while
	// the rest are shifted down in its place
	// Compute current register index
	MOVQ DI, R13
	SHRQ $+36, R13

	// Increment current output sample index and compute next register index
	ADDQ R9, DI
	MOVQ DI, R14
	SHRQ $+36, R14

	// Store and shift registers if a register transition has happened
	CMPQ    R13, R14
	JE      no_store
	VMOVUPS Z0, (AX)(R10*4)
	VMOVUPS Z1, Z0
	VXORPS  Z1, Z1, Z1

no_store:
	// Update and wrap the vector-aligned output index
	MOVQ R14, R10
	SHLQ $+4, R10
	ANDQ R8, R10

	// Update and wrap coefficient index
	XORQ R13, R13
	ADDQ $+64, BX
	CMPQ BX, SI

	// Wrap phase counter - SUB changes flags so do this after to avoid clobbering Compare result
	CMOVQGE SI, R13
	SUBQ    R13, BX
	ADDQ    $0x00000001, R11
	JMP     In0

In0end:
	// Store each partially accumulated vector to the output slice
	// taking care to wrap into output ringbuffer
	VMOVUPS Z0, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z1, (AX)(R10*4)
	VZEROUPPER

	// Return the latest phase and output index for reuse in future calls
	MOVQ BX, CoefIdxOut+96(FP)
	MOVQ DI, OutIdxOut+104(FP)
	RET

// func ResampleFixedF32_16x3(Out []float32, In []float32, Coefs []float32, CoefIdx int, OutIdx int, OutStep int) (CoefIdxOut int, OutIdxOut int)
// Requires: AVX, AVX512DQ, AVX512F, CMOV
TEXT ·ResampleFixedF32_16x3(SB), NOSPLIT, $0-112
	MOVQ Out_base+0(FP), AX
	MOVQ In_base+24(FP), CX
	MOVQ Coefs_base+48(FP), DX
	MOVQ CoefIdx+72(FP), BX
	MOVQ Coefs_len+56(FP), SI
	MOVQ OutIdx+80(FP), DI
	MOVQ Out_len+8(FP), R8
	SUBQ $+1, R8
	MOVQ OutStep+88(FP), R9

	// Reload previous partially accumulated output samples
	// Compute output vector index as 
	// (fixedPointIndex / fixedPointScale / vectorLength * vectorLength) % outBufferLength
	// wraps within the output buffer and quantizes the nearest vector register multiple
	MOVQ DI, R10
	SHRQ $+36, R10
	SHLQ $+4, R10
	ANDQ R8, R10

	// Re-load partially accumulated output samples, wrapping ringbuffer
	MOVQ    R10, R11
	VMOVUPS (AX)(R11*4), Z0
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z1
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z2

	// For each input sample:
	XORQ R11, R11
	MOVQ In_len+32(FP), R12
	SUBQ $0x00000001, R12

In0:
	CMPQ R12, R11
	JL   In0end

	// The coefficient load index is (filtertaps+padding) * wrappedInputIndex
	// + (outAlignedIdx*vectorLength - outSampleIdx
	// This loads each coefficient set within a block with proper zero padding
	// so that multiple coefficient sets can be accumulated into one set of registers,
	// offset by the proper number of in-register samples
	// Broadcast the current input sample and contribute and accumulate its output-phase-specific-coefficient-scaled individual contribution to every output sample in range
	VBROADCASTSS (CX)(R11*4), Z3
	VFMADD231PS  (DX)(BX*4), Z3, Z0
	VFMADD231PS  64(DX)(BX*4), Z3, Z1
	VFMADD231PS  128(DX)(BX*4), Z3, Z2

	// If incrementing the output index crosses a multiple of vectorLength,
	// the lowest register is completely accumulated and can be stored while
	// the rest are shifted down in its place
	// Compute current register index
	MOVQ DI, R13
	SHRQ $+36, R13

	// Increment current output sample index and compute next register index
	ADDQ R9, DI
	MOVQ DI, R14
	SHRQ $+36, R14

	// Store and shift registers if a register transition has happened
	CMPQ    R13, R14
	JE      no_store
	VMOVUPS Z0, (AX)(R10*4)
	VMOVUPS Z1, Z0
	VMOVUPS Z2, Z1
	VXORPS  Z2, Z2, Z2

no_store:
	// Update and wrap the vector-aligned output index
	MOVQ R14, R10
	SHLQ $+4, R10
	ANDQ R8, R10

	// Update and wrap coefficient index
	XORQ R13, R13
	ADDQ $+80, BX
	CMPQ BX, SI

	// Wrap phase counter - SUB changes flags so do this after to avoid clobbering Compare result
	CMOVQGE SI, R13
	SUBQ    R13, BX
	ADDQ    $0x00000001, R11
	JMP     In0

In0end:
	// Store each partially accumulated vector to the output slice
	// taking care to wrap into output ringbuffer
	VMOVUPS Z0, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z1, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z2, (AX)(R10*4)
	VZEROUPPER

	// Return the latest phase and output index for reuse in future calls
	MOVQ BX, CoefIdxOut+96(FP)
	MOVQ DI, OutIdxOut+104(FP)
	RET

// func ResampleFixedF32_16x4(Out []float32, In []float32, Coefs []float32, CoefIdx int, OutIdx int, OutStep int) (CoefIdxOut int, OutIdxOut int)
// Requires: AVX, AVX512DQ, AVX512F, CMOV
TEXT ·ResampleFixedF32_16x4(SB), NOSPLIT, $0-112
	MOVQ Out_base+0(FP), AX
	MOVQ In_base+24(FP), CX
	MOVQ Coefs_base+48(FP), DX
	MOVQ CoefIdx+72(FP), BX
	MOVQ Coefs_len+56(FP), SI
	MOVQ OutIdx+80(FP), DI
	MOVQ Out_len+8(FP), R8
	SUBQ $+1, R8
	MOVQ OutStep+88(FP), R9

	// Reload previous partially accumulated output samples
	// Compute output vector index as 
	// (fixedPointIndex / fixedPointScale / vectorLength * vectorLength) % outBufferLength
	// wraps within the output buffer and quantizes the nearest vector register multiple
	MOVQ DI, R10
	SHRQ $+36, R10
	SHLQ $+4, R10
	ANDQ R8, R10

	// Re-load partially accumulated output samples, wrapping ringbuffer
	MOVQ    R10, R11
	VMOVUPS (AX)(R11*4), Z0
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z1
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z2
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z3

	// For each input sample:
	XORQ R11, R11
	MOVQ In_len+32(FP), R12
	SUBQ $0x00000001, R12

In0:
	CMPQ R12, R11
	JL   In0end

	// The coefficient load index is (filtertaps+padding) * wrappedInputIndex
	// + (outAlignedIdx*vectorLength - outSampleIdx
	// This loads each coefficient set within a block with proper zero padding
	// so that multiple coefficient sets can be accumulated into one set of registers,
	// offset by the proper number of in-register samples
	// Broadcast the current input sample and contribute and accumulate its output-phase-specific-coefficient-scaled individual contribution to every output sample in range
	VBROADCASTSS (CX)(R11*4), Z4
	VFMADD231PS  (DX)(BX*4), Z4, Z0
	VFMADD231PS  64(DX)(BX*4), Z4, Z1
	VFMADD231PS  128(DX)(BX*4), Z4, Z2
	VFMADD231PS  192(DX)(BX*4), Z4, Z3

	// If incrementing the output index crosses a multiple of vectorLength,
	// the lowest register is completely accumulated and can be stored while
	// the rest are shifted down in its place
	// Compute current register index
	MOVQ DI, R13
	SHRQ $+36, R13

	// Increment current output sample index and compute next register index
	ADDQ R9, DI
	MOVQ DI, R14
	SHRQ $+36, R14

	// Store and shift registers if a register transition has happened
	CMPQ    R13, R14
	JE      no_store
	VMOVUPS Z0, (AX)(R10*4)
	VMOVUPS Z1, Z0
	VMOVUPS Z2, Z1
	VMOVUPS Z3, Z2
	VXORPS  Z3, Z3, Z3

no_store:
	// Update and wrap the vector-aligned output index
	MOVQ R14, R10
	SHLQ $+4, R10
	ANDQ R8, R10

	// Update and wrap coefficient index
	XORQ R13, R13
	ADDQ $+96, BX
	CMPQ BX, SI

	// Wrap phase counter - SUB changes flags so do this after to avoid clobbering Compare result
	CMOVQGE SI, R13
	SUBQ    R13, BX
	ADDQ    $0x00000001, R11
	JMP     In0

In0end:
	// Store each partially accumulated vector to the output slice
	// taking care to wrap into output ringbuffer
	VMOVUPS Z0, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z1, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z2, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z3, (AX)(R10*4)
	VZEROUPPER

	// Return the latest phase and output index for reuse in future calls
	MOVQ BX, CoefIdxOut+96(FP)
	MOVQ DI, OutIdxOut+104(FP)
	RET

// func ResampleFixedF32_16x5(Out []float32, In []float32, Coefs []float32, CoefIdx int, OutIdx int, OutStep int) (CoefIdxOut int, OutIdxOut int)
// Requires: AVX, AVX512DQ, AVX512F, CMOV
TEXT ·ResampleFixedF32_16x5(SB), NOSPLIT, $0-112
	MOVQ Out_base+0(FP), AX
	MOVQ In_base+24(FP), CX
	MOVQ Coefs_base+48(FP), DX
	MOVQ CoefIdx+72(FP), BX
	MOVQ Coefs_len+56(FP), SI
	MOVQ OutIdx+80(FP), DI
	MOVQ Out_len+8(FP), R8
	SUBQ $+1, R8
	MOVQ OutStep+88(FP), R9

	// Reload previous partially accumulated output samples
	// Compute output vector index as 
	// (fixedPointIndex / fixedPointScale / vectorLength * vectorLength) % outBufferLength
	// wraps within the output buffer and quantizes the nearest vector register multiple
	MOVQ DI, R10
	SHRQ $+36, R10
	SHLQ $+4, R10
	ANDQ R8, R10

	// Re-load partially accumulated output samples, wrapping ringbuffer
	MOVQ    R10, R11
	VMOVUPS (AX)(R11*4), Z0
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z1
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z2
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z3
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z4

	// For each input sample:
	XORQ R11, R11
	MOVQ In_len+32(FP), R12
	SUBQ $0x00000001, R12

In0:
	CMPQ R12, R11
	JL   In0end

	// The coefficient load index is (filtertaps+padding) * wrappedInputIndex
	// + (outAlignedIdx*vectorLength - outSampleIdx
	// This loads each coefficient set within a block with proper zero padding
	// so that multiple coefficient sets can be accumulated into one set of registers,
	// offset by the proper number of in-register samples
	// Broadcast the current input sample and contribute and accumulate its output-phase-specific-coefficient-scaled individual contribution to every output sample in range
	VBROADCASTSS (CX)(R11*4), Z5
	VFMADD231PS  (DX)(BX*4), Z5, Z0
	VFMADD231PS  64(DX)(BX*4), Z5, Z1
	VFMADD231PS  128(DX)(BX*4), Z5, Z2
	VFMADD231PS  192(DX)(BX*4), Z5, Z3
	VFMADD231PS  256(DX)(BX*4), Z5, Z4

	// If incrementing the output index crosses a multiple of vectorLength,
	// the lowest register is completely accumulated and can be stored while
	// the rest are shifted down in its place
	// Compute current register index
	MOVQ DI, R13
	SHRQ $+36, R13

	// Increment current output sample index and compute next register index
	ADDQ R9, DI
	MOVQ DI, R14
	SHRQ $+36, R14

	// Store and shift registers if a register transition has happened
	CMPQ    R13, R14
	JE      no_store
	VMOVUPS Z0, (AX)(R10*4)
	VMOVUPS Z1, Z0
	VMOVUPS Z2, Z1
	VMOVUPS Z3, Z2
	VMOVUPS Z4, Z3
	VXORPS  Z4, Z4, Z4

no_store:
	// Update and wrap the vector-aligned output index
	MOVQ R14, R10
	SHLQ $+4, R10
	ANDQ R8, R10

	// Update and wrap coefficient index
	XORQ R13, R13
	ADDQ $+112, BX
	CMPQ BX, SI

	// Wrap phase counter - SUB changes flags so do this after to avoid clobbering Compare result
	CMOVQGE SI, R13
	SUBQ    R13, BX
	ADDQ    $0x00000001, R11
	JMP     In0

In0end:
	// Store each partially accumulated vector to the output slice
	// taking care to wrap into output ringbuffer
	VMOVUPS Z0, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z1, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z2, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z3, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z4, (AX)(R10*4)
	VZEROUPPER

	// Return the latest phase and output index for reuse in future calls
	MOVQ BX, CoefIdxOut+96(FP)
	MOVQ DI, OutIdxOut+104(FP)
	RET

// func ResampleFixedF32_16x6(Out []float32, In []float32, Coefs []float32, CoefIdx int, OutIdx int, OutStep int) (CoefIdxOut int, OutIdxOut int)
// Requires: AVX, AVX512DQ, AVX512F, CMOV
TEXT ·ResampleFixedF32_16x6(SB), NOSPLIT, $0-112
	MOVQ Out_base+0(FP), AX
	MOVQ In_base+24(FP), CX
	MOVQ Coefs_base+48(FP), DX
	MOVQ CoefIdx+72(FP), BX
	MOVQ Coefs_len+56(FP), SI
	MOVQ OutIdx+80(FP), DI
	MOVQ Out_len+8(FP), R8
	SUBQ $+1, R8
	MOVQ OutStep+88(FP), R9

	// Reload previous partially accumulated output samples
	// Compute output vector index as 
	// (fixedPointIndex / fixedPointScale / vectorLength * vectorLength) % outBufferLength
	// wraps within the output buffer and quantizes the nearest vector register multiple
	MOVQ DI, R10
	SHRQ $+36, R10
	SHLQ $+4, R10
	ANDQ R8, R10

	// Re-load partially accumulated output samples, wrapping ringbuffer
	MOVQ    R10, R11
	VMOVUPS (AX)(R11*4), Z0
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z1
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z2
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z3
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z4
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z5

	// For each input sample:
	XORQ R11, R11
	MOVQ In_len+32(FP), R12
	SUBQ $0x00000001, R12

In0:
	CMPQ R12, R11
	JL   In0end

	// The coefficient load index is (filtertaps+padding) * wrappedInputIndex
	// + (outAlignedIdx*vectorLength - outSampleIdx
	// This loads each coefficient set within a block with proper zero padding
	// so that multiple coefficient sets can be accumulated into one set of registers,
	// offset by the proper number of in-register samples
	// Broadcast the current input sample and contribute and accumulate its output-phase-specific-coefficient-scaled individual contribution to every output sample in range
	VBROADCASTSS (CX)(R11*4), Z6
	VFMADD231PS  (DX)(BX*4), Z6, Z0
	VFMADD231PS  64(DX)(BX*4), Z6, Z1
	VFMADD231PS  128(DX)(BX*4), Z6, Z2
	VFMADD231PS  192(DX)(BX*4), Z6, Z3
	VFMADD231PS  256(DX)(BX*4), Z6, Z4
	VFMADD231PS  320(DX)(BX*4), Z6, Z5

	// If incrementing the output index crosses a multiple of vectorLength,
	// the lowest register is completely accumulated and can be stored while
	// the rest are shifted down in its place
	// Compute current register index
	MOVQ DI, R13
	SHRQ $+36, R13

	// Increment current output sample index and compute next register index
	ADDQ R9, DI
	MOVQ DI, R14
	SHRQ $+36, R14

	// Store and shift registers if a register transition has happened
	CMPQ    R13, R14
	JE      no_store
	VMOVUPS Z0, (AX)(R10*4)
	VMOVUPS Z1, Z0
	VMOVUPS Z2, Z1
	VMOVUPS Z3, Z2
	VMOVUPS Z4, Z3
	VMOVUPS Z5, Z4
	VXORPS  Z5, Z5, Z5

no_store:
	// Update and wrap the vector-aligned output index
	MOVQ R14, R10
	SHLQ $+4, R10
	ANDQ R8, R10

	// Update and wrap coefficient index
	XORQ R13, R13
	ADDQ $+128, BX
	CMPQ BX, SI

	// Wrap phase counter - SUB changes flags so do this after to avoid clobbering Compare result
	CMOVQGE SI, R13
	SUBQ    R13, BX
	ADDQ    $0x00000001, R11
	JMP     In0

In0end:
	// Store each partially accumulated vector to the output slice
	// taking care to wrap into output ringbuffer
	VMOVUPS Z0, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z1, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z2, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z3, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z4, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z5, (AX)(R10*4)
	VZEROUPPER

	// Return the latest phase and output index for reuse in future calls
	MOVQ BX, CoefIdxOut+96(FP)
	MOVQ DI, OutIdxOut+104(FP)
	RET

// func ResampleFixedF32_16x7(Out []float32, In []float32, Coefs []float32, CoefIdx int, OutIdx int, OutStep int) (CoefIdxOut int, OutIdxOut int)
// Requires: AVX, AVX512DQ, AVX512F, CMOV
TEXT ·ResampleFixedF32_16x7(SB), NOSPLIT, $0-112
	MOVQ Out_base+0(FP), AX
	MOVQ In_base+24(FP), CX
	MOVQ Coefs_base+48(FP), DX
	MOVQ CoefIdx+72(FP), BX
	MOVQ Coefs_len+56(FP), SI
	MOVQ OutIdx+80(FP), DI
	MOVQ Out_len+8(FP), R8
	SUBQ $+1, R8
	MOVQ OutStep+88(FP), R9

	// Reload previous partially accumulated output samples
	// Compute output vector index as 
	// (fixedPointIndex / fixedPointScale / vectorLength * vectorLength) % outBufferLength
	// wraps within the output buffer and quantizes the nearest vector register multiple
	MOVQ DI, R10
	SHRQ $+36, R10
	SHLQ $+4, R10
	ANDQ R8, R10

	// Re-load partially accumulated output samples, wrapping ringbuffer
	MOVQ    R10, R11
	VMOVUPS (AX)(R11*4), Z0
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z1
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z2
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z3
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z4
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z5
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z6

	// For each input sample:
	XORQ R11, R11
	MOVQ In_len+32(FP), R12
	SUBQ $0x00000001, R12

In0:
	CMPQ R12, R11
	JL   In0end

	// The coefficient load index is (filtertaps+padding) * wrappedInputIndex
	// + (outAlignedIdx*vectorLength - outSampleIdx
	// This loads each coefficient set within a block with proper zero padding
	// so that multiple coefficient sets can be accumulated into one set of registers,
	// offset by the proper number of in-register samples
	// Broadcast the current input sample and contribute and accumulate its output-phase-specific-coefficient-scaled individual contribution to every output sample in range
	VBROADCASTSS (CX)(R11*4), Z7
	VFMADD231PS  (DX)(BX*4), Z7, Z0
	VFMADD231PS  64(DX)(BX*4), Z7, Z1
	VFMADD231PS  128(DX)(BX*4), Z7, Z2
	VFMADD231PS  192(DX)(BX*4), Z7, Z3
	VFMADD231PS  256(DX)(BX*4), Z7, Z4
	VFMADD231PS  320(DX)(BX*4), Z7, Z5
	VFMADD231PS  384(DX)(BX*4), Z7, Z6

	// If incrementing the output index crosses a multiple of vectorLength,
	// the lowest register is completely accumulated and can be stored while
	// the rest are shifted down in its place
	// Compute current register index
	MOVQ DI, R13
	SHRQ $+36, R13

	// Increment current output sample index and compute next register index
	ADDQ R9, DI
	MOVQ DI, R14
	SHRQ $+36, R14

	// Store and shift registers if a register transition has happened
	CMPQ    R13, R14
	JE      no_store
	VMOVUPS Z0, (AX)(R10*4)
	VMOVUPS Z1, Z0
	VMOVUPS Z2, Z1
	VMOVUPS Z3, Z2
	VMOVUPS Z4, Z3
	VMOVUPS Z5, Z4
	VMOVUPS Z6, Z5
	VXORPS  Z6, Z6, Z6

no_store:
	// Update and wrap the vector-aligned output index
	MOVQ R14, R10
	SHLQ $+4, R10
	ANDQ R8, R10

	// Update and wrap coefficient index
	XORQ R13, R13
	ADDQ $+144, BX
	CMPQ BX, SI

	// Wrap phase counter - SUB changes flags so do this after to avoid clobbering Compare result
	CMOVQGE SI, R13
	SUBQ    R13, BX
	ADDQ    $0x00000001, R11
	JMP     In0

In0end:
	// Store each partially accumulated vector to the output slice
	// taking care to wrap into output ringbuffer
	VMOVUPS Z0, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z1, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z2, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z3, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z4, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z5, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z6, (AX)(R10*4)
	VZEROUPPER

	// Return the latest phase and output index for reuse in future calls
	MOVQ BX, CoefIdxOut+96(FP)
	MOVQ DI, OutIdxOut+104(FP)
	RET

// func ResampleFixedF32_16x8(Out []float32, In []float32, Coefs []float32, CoefIdx int, OutIdx int, OutStep int) (CoefIdxOut int, OutIdxOut int)
// Requires: AVX, AVX512DQ, AVX512F, CMOV
TEXT ·ResampleFixedF32_16x8(SB), NOSPLIT, $0-112
	MOVQ Out_base+0(FP), AX
	MOVQ In_base+24(FP), CX
	MOVQ Coefs_base+48(FP), DX
	MOVQ CoefIdx+72(FP), BX
	MOVQ Coefs_len+56(FP), SI
	MOVQ OutIdx+80(FP), DI
	MOVQ Out_len+8(FP), R8
	SUBQ $+1, R8
	MOVQ OutStep+88(FP), R9

	// Reload previous partially accumulated output samples
	// Compute output vector index as 
	// (fixedPointIndex / fixedPointScale / vectorLength * vectorLength) % outBufferLength
	// wraps within the output buffer and quantizes the nearest vector register multiple
	MOVQ DI, R10
	SHRQ $+36, R10
	SHLQ $+4, R10
	ANDQ R8, R10

	// Re-load partially accumulated output samples, wrapping ringbuffer
	MOVQ    R10, R11
	VMOVUPS (AX)(R11*4), Z0
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z1
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z2
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z3
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z4
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z5
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z6
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z7

	// For each input sample:
	XORQ R11, R11
	MOVQ In_len+32(FP), R12
	SUBQ $0x00000001, R12

In0:
	CMPQ R12, R11
	JL   In0end

	// The coefficient load index is (filtertaps+padding) * wrappedInputIndex
	// + (outAlignedIdx*vectorLength - outSampleIdx
	// This loads each coefficient set within a block with proper zero padding
	// so that multiple coefficient sets can be accumulated into one set of registers,
	// offset by the proper number of in-register samples
	// Broadcast the current input sample and contribute and accumulate its output-phase-specific-coefficient-scaled individual contribution to every output sample in range
	VBROADCASTSS (CX)(R11*4), Z8
	VFMADD231PS  (DX)(BX*4), Z8, Z0
	VFMADD231PS  64(DX)(BX*4), Z8, Z1
	VFMADD231PS  128(DX)(BX*4), Z8, Z2
	VFMADD231PS  192(DX)(BX*4), Z8, Z3
	VFMADD231PS  256(DX)(BX*4), Z8, Z4
	VFMADD231PS  320(DX)(BX*4), Z8, Z5
	VFMADD231PS  384(DX)(BX*4), Z8, Z6
	VFMADD231PS  448(DX)(BX*4), Z8, Z7

	// If incrementing the output index crosses a multiple of vectorLength,
	// the lowest register is completely accumulated and can be stored while
	// the rest are shifted down in its place
	// Compute current register index
	MOVQ DI, R13
	SHRQ $+36, R13

	// Increment current output sample index and compute next register index
	ADDQ R9, DI
	MOVQ DI, R14
	SHRQ $+36, R14

	// Store and shift registers if a register transition has happened
	CMPQ    R13, R14
	JE      no_store
	VMOVUPS Z0, (AX)(R10*4)
	VMOVUPS Z1, Z0
	VMOVUPS Z2, Z1
	VMOVUPS Z3, Z2
	VMOVUPS Z4, Z3
	VMOVUPS Z5, Z4
	VMOVUPS Z6, Z5
	VMOVUPS Z7, Z6
	VXORPS  Z7, Z7, Z7

no_store:
	// Update and wrap the vector-aligned output index
	MOVQ R14, R10
	SHLQ $+4, R10
	ANDQ R8, R10

	// Update and wrap coefficient index
	XORQ R13, R13
	ADDQ $+160, BX
	CMPQ BX, SI

	// Wrap phase counter - SUB changes flags so do this after to avoid clobbering Compare result
	CMOVQGE SI, R13
	SUBQ    R13, BX
	ADDQ    $0x00000001, R11
	JMP     In0

In0end:
	// Store each partially accumulated vector to the output slice
	// taking care to wrap into output ringbuffer
	VMOVUPS Z0, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z1, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z2, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z3, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z4, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z5, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z6, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z7, (AX)(R10*4)
	VZEROUPPER

	// Return the latest phase and output index for reuse in future calls
	MOVQ BX, CoefIdxOut+96(FP)
	MOVQ DI, OutIdxOut+104(FP)
	RET

// func ResampleFixedF32_16x9(Out []float32, In []float32, Coefs []float32, CoefIdx int, OutIdx int, OutStep int) (CoefIdxOut int, OutIdxOut int)
// Requires: AVX, AVX512DQ, AVX512F, CMOV
TEXT ·ResampleFixedF32_16x9(SB), NOSPLIT, $0-112
	MOVQ Out_base+0(FP), AX
	MOVQ In_base+24(FP), CX
	MOVQ Coefs_base+48(FP), DX
	MOVQ CoefIdx+72(FP), BX
	MOVQ Coefs_len+56(FP), SI
	MOVQ OutIdx+80(FP), DI
	MOVQ Out_len+8(FP), R8
	SUBQ $+1, R8
	MOVQ OutStep+88(FP), R9

	// Reload previous partially accumulated output samples
	// Compute output vector index as 
	// (fixedPointIndex / fixedPointScale / vectorLength * vectorLength) % outBufferLength
	// wraps within the output buffer and quantizes the nearest vector register multiple
	MOVQ DI, R10
	SHRQ $+36, R10
	SHLQ $+4, R10
	ANDQ R8, R10

	// Re-load partially accumulated output samples, wrapping ringbuffer
	MOVQ    R10, R11
	VMOVUPS (AX)(R11*4), Z0
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z1
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z2
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z3
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z4
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z5
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z6
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z7
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z8

	// For each input sample:
	XORQ R11, R11
	MOVQ In_len+32(FP), R12
	SUBQ $0x00000001, R12

In0:
	CMPQ R12, R11
	JL   In0end

	// The coefficient load index is (filtertaps+padding) * wrappedInputIndex
	// + (outAlignedIdx*vectorLength - outSampleIdx
	// This loads each coefficient set within a block with proper zero padding
	// so that multiple coefficient sets can be accumulated into one set of registers,
	// offset by the proper number of in-register samples
	// Broadcast the current input sample and contribute and accumulate its output-phase-specific-coefficient-scaled individual contribution to every output sample in range
	VBROADCASTSS (CX)(R11*4), Z9
	VFMADD231PS  (DX)(BX*4), Z9, Z0
	VFMADD231PS  64(DX)(BX*4), Z9, Z1
	VFMADD231PS  128(DX)(BX*4), Z9, Z2
	VFMADD231PS  192(DX)(BX*4), Z9, Z3
	VFMADD231PS  256(DX)(BX*4), Z9, Z4
	VFMADD231PS  320(DX)(BX*4), Z9, Z5
	VFMADD231PS  384(DX)(BX*4), Z9, Z6
	VFMADD231PS  448(DX)(BX*4), Z9, Z7
	VFMADD231PS  512(DX)(BX*4), Z9, Z8

	// If incrementing the output index crosses a multiple of vectorLength,
	// the lowest register is completely accumulated and can be stored while
	// the rest are shifted down in its place
	// Compute current register index
	MOVQ DI, R13
	SHRQ $+36, R13

	// Increment current output sample index and compute next register index
	ADDQ R9, DI
	MOVQ DI, R14
	SHRQ $+36, R14

	// Store and shift registers if a register transition has happened
	CMPQ    R13, R14
	JE      no_store
	VMOVUPS Z0, (AX)(R10*4)
	VMOVUPS Z1, Z0
	VMOVUPS Z2, Z1
	VMOVUPS Z3, Z2
	VMOVUPS Z4, Z3
	VMOVUPS Z5, Z4
	VMOVUPS Z6, Z5
	VMOVUPS Z7, Z6
	VMOVUPS Z8, Z7
	VXORPS  Z8, Z8, Z8

no_store:
	// Update and wrap the vector-aligned output index
	MOVQ R14, R10
	SHLQ $+4, R10
	ANDQ R8, R10

	// Update and wrap coefficient index
	XORQ R13, R13
	ADDQ $+176, BX
	CMPQ BX, SI

	// Wrap phase counter - SUB changes flags so do this after to avoid clobbering Compare result
	CMOVQGE SI, R13
	SUBQ    R13, BX
	ADDQ    $0x00000001, R11
	JMP     In0

In0end:
	// Store each partially accumulated vector to the output slice
	// taking care to wrap into output ringbuffer
	VMOVUPS Z0, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z1, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z2, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z3, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z4, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z5, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z6, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z7, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z8, (AX)(R10*4)
	VZEROUPPER

	// Return the latest phase and output index for reuse in future calls
	MOVQ BX, CoefIdxOut+96(FP)
	MOVQ DI, OutIdxOut+104(FP)
	RET

// func ResampleFixedF32_16x10(Out []float32, In []float32, Coefs []float32, CoefIdx int, OutIdx int, OutStep int) (CoefIdxOut int, OutIdxOut int)
// Requires: AVX, AVX512DQ, AVX512F, CMOV
TEXT ·ResampleFixedF32_16x10(SB), NOSPLIT, $0-112
	MOVQ Out_base+0(FP), AX
	MOVQ In_base+24(FP), CX
	MOVQ Coefs_base+48(FP), DX
	MOVQ CoefIdx+72(FP), BX
	MOVQ Coefs_len+56(FP), SI
	MOVQ OutIdx+80(FP), DI
	MOVQ Out_len+8(FP), R8
	SUBQ $+1, R8
	MOVQ OutStep+88(FP), R9

	// Reload previous partially accumulated output samples
	// Compute output vector index as 
	// (fixedPointIndex / fixedPointScale / vectorLength * vectorLength) % outBufferLength
	// wraps within the output buffer and quantizes the nearest vector register multiple
	MOVQ DI, R10
	SHRQ $+36, R10
	SHLQ $+4, R10
	ANDQ R8, R10

	// Re-load partially accumulated output samples, wrapping ringbuffer
	MOVQ    R10, R11
	VMOVUPS (AX)(R11*4), Z0
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z1
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z2
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z3
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z4
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z5
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z6
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z7
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z8
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z9

	// For each input sample:
	XORQ R11, R11
	MOVQ In_len+32(FP), R12
	SUBQ $0x00000001, R12

In0:
	CMPQ R12, R11
	JL   In0end

	// The coefficient load index is (filtertaps+padding) * wrappedInputIndex
	// + (outAlignedIdx*vectorLength - outSampleIdx
	// This loads each coefficient set within a block with proper zero padding
	// so that multiple coefficient sets can be accumulated into one set of registers,
	// offset by the proper number of in-register samples
	// Broadcast the current input sample and contribute and accumulate its output-phase-specific-coefficient-scaled individual contribution to every output sample in range
	VBROADCASTSS (CX)(R11*4), Z10
	VFMADD231PS  (DX)(BX*4), Z10, Z0
	VFMADD231PS  64(DX)(BX*4), Z10, Z1
	VFMADD231PS  128(DX)(BX*4), Z10, Z2
	VFMADD231PS  192(DX)(BX*4), Z10, Z3
	VFMADD231PS  256(DX)(BX*4), Z10, Z4
	VFMADD231PS  320(DX)(BX*4), Z10, Z5
	VFMADD231PS  384(DX)(BX*4), Z10, Z6
	VFMADD231PS  448(DX)(BX*4), Z10, Z7
	VFMADD231PS  512(DX)(BX*4), Z10, Z8
	VFMADD231PS  576(DX)(BX*4), Z10, Z9

	// If incrementing the output index crosses a multiple of vectorLength,
	// the lowest register is completely accumulated and can be stored while
	// the rest are shifted down in its place
	// Compute current register index
	MOVQ DI, R13
	SHRQ $+36, R13

	// Increment current output sample index and compute next register index
	ADDQ R9, DI
	MOVQ DI, R14
	SHRQ $+36, R14

	// Store and shift registers if a register transition has happened
	CMPQ    R13, R14
	JE      no_store
	VMOVUPS Z0, (AX)(R10*4)
	VMOVUPS Z1, Z0
	VMOVUPS Z2, Z1
	VMOVUPS Z3, Z2
	VMOVUPS Z4, Z3
	VMOVUPS Z5, Z4
	VMOVUPS Z6, Z5
	VMOVUPS Z7, Z6
	VMOVUPS Z8, Z7
	VMOVUPS Z9, Z8
	VXORPS  Z9, Z9, Z9

no_store:
	// Update and wrap the vector-aligned output index
	MOVQ R14, R10
	SHLQ $+4, R10
	ANDQ R8, R10

	// Update and wrap coefficient index
	XORQ R13, R13
	ADDQ $+192, BX
	CMPQ BX, SI

	// Wrap phase counter - SUB changes flags so do this after to avoid clobbering Compare result
	CMOVQGE SI, R13
	SUBQ    R13, BX
	ADDQ    $0x00000001, R11
	JMP     In0

In0end:
	// Store each partially accumulated vector to the output slice
	// taking care to wrap into output ringbuffer
	VMOVUPS Z0, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z1, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z2, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z3, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z4, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z5, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z6, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z7, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z8, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z9, (AX)(R10*4)
	VZEROUPPER

	// Return the latest phase and output index for reuse in future calls
	MOVQ BX, CoefIdxOut+96(FP)
	MOVQ DI, OutIdxOut+104(FP)
	RET

// func ResampleFixedF32_16x11(Out []float32, In []float32, Coefs []float32, CoefIdx int, OutIdx int, OutStep int) (CoefIdxOut int, OutIdxOut int)
// Requires: AVX, AVX512DQ, AVX512F, CMOV
TEXT ·ResampleFixedF32_16x11(SB), NOSPLIT, $0-112
	MOVQ Out_base+0(FP), AX
	MOVQ In_base+24(FP), CX
	MOVQ Coefs_base+48(FP), DX
	MOVQ CoefIdx+72(FP), BX
	MOVQ Coefs_len+56(FP), SI
	MOVQ OutIdx+80(FP), DI
	MOVQ Out_len+8(FP), R8
	SUBQ $+1, R8
	MOVQ OutStep+88(FP), R9

	// Reload previous partially accumulated output samples
	// Compute output vector index as 
	// (fixedPointIndex / fixedPointScale / vectorLength * vectorLength) % outBufferLength
	// wraps within the output buffer and quantizes the nearest vector register multiple
	MOVQ DI, R10
	SHRQ $+36, R10
	SHLQ $+4, R10
	ANDQ R8, R10

	// Re-load partially accumulated output samples, wrapping ringbuffer
	MOVQ    R10, R11
	VMOVUPS (AX)(R11*4), Z0
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z1
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z2
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z3
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z4
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z5
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z6
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z7
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z8
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z9
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z10

	// For each input sample:
	XORQ R11, R11
	MOVQ In_len+32(FP), R12
	SUBQ $0x00000001, R12

In0:
	CMPQ R12, R11
	JL   In0end

	// The coefficient load index is (filtertaps+padding) * wrappedInputIndex
	// + (outAlignedIdx*vectorLength - outSampleIdx
	// This loads each coefficient set within a block with proper zero padding
	// so that multiple coefficient sets can be accumulated into one set of registers,
	// offset by the proper number of in-register samples
	// Broadcast the current input sample and contribute and accumulate its output-phase-specific-coefficient-scaled individual contribution to every output sample in range
	VBROADCASTSS (CX)(R11*4), Z11
	VFMADD231PS  (DX)(BX*4), Z11, Z0
	VFMADD231PS  64(DX)(BX*4), Z11, Z1
	VFMADD231PS  128(DX)(BX*4), Z11, Z2
	VFMADD231PS  192(DX)(BX*4), Z11, Z3
	VFMADD231PS  256(DX)(BX*4), Z11, Z4
	VFMADD231PS  320(DX)(BX*4), Z11, Z5
	VFMADD231PS  384(DX)(BX*4), Z11, Z6
	VFMADD231PS  448(DX)(BX*4), Z11, Z7
	VFMADD231PS  512(DX)(BX*4), Z11, Z8
	VFMADD231PS  576(DX)(BX*4), Z11, Z9
	VFMADD231PS  640(DX)(BX*4), Z11, Z10

	// If incrementing the output index crosses a multiple of vectorLength,
	// the lowest register is completely accumulated and can be stored while
	// the rest are shifted down in its place
	// Compute current register index
	MOVQ DI, R13
	SHRQ $+36, R13

	// Increment current output sample index and compute next register index
	ADDQ R9, DI
	MOVQ DI, R14
	SHRQ $+36, R14

	// Store and shift registers if a register transition has happened
	CMPQ    R13, R14
	JE      no_store
	VMOVUPS Z0, (AX)(R10*4)
	VMOVUPS Z1, Z0
	VMOVUPS Z2, Z1
	VMOVUPS Z3, Z2
	VMOVUPS Z4, Z3
	VMOVUPS Z5, Z4
	VMOVUPS Z6, Z5
	VMOVUPS Z7, Z6
	VMOVUPS Z8, Z7
	VMOVUPS Z9, Z8
	VMOVUPS Z10, Z9
	VXORPS  Z10, Z10, Z10

no_store:
	// Update and wrap the vector-aligned output index
	MOVQ R14, R10
	SHLQ $+4, R10
	ANDQ R8, R10

	// Update and wrap coefficient index
	XORQ R13, R13
	ADDQ $+208, BX
	CMPQ BX, SI

	// Wrap phase counter - SUB changes flags so do this after to avoid clobbering Compare result
	CMOVQGE SI, R13
	SUBQ    R13, BX
	ADDQ    $0x00000001, R11
	JMP     In0

In0end:
	// Store each partially accumulated vector to the output slice
	// taking care to wrap into output ringbuffer
	VMOVUPS Z0, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z1, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z2, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z3, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z4, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z5, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z6, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z7, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z8, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z9, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z10, (AX)(R10*4)
	VZEROUPPER

	// Return the latest phase and output index for reuse in future calls
	MOVQ BX, CoefIdxOut+96(FP)
	MOVQ DI, OutIdxOut+104(FP)
	RET

// func ResampleFixedF32_16x12(Out []float32, In []float32, Coefs []float32, CoefIdx int, OutIdx int, OutStep int) (CoefIdxOut int, OutIdxOut int)
// Requires: AVX, AVX512DQ, AVX512F, CMOV
TEXT ·ResampleFixedF32_16x12(SB), NOSPLIT, $0-112
	MOVQ Out_base+0(FP), AX
	MOVQ In_base+24(FP), CX
	MOVQ Coefs_base+48(FP), DX
	MOVQ CoefIdx+72(FP), BX
	MOVQ Coefs_len+56(FP), SI
	MOVQ OutIdx+80(FP), DI
	MOVQ Out_len+8(FP), R8
	SUBQ $+1, R8
	MOVQ OutStep+88(FP), R9

	// Reload previous partially accumulated output samples
	// Compute output vector index as 
	// (fixedPointIndex / fixedPointScale / vectorLength * vectorLength) % outBufferLength
	// wraps within the output buffer and quantizes the nearest vector register multiple
	MOVQ DI, R10
	SHRQ $+36, R10
	SHLQ $+4, R10
	ANDQ R8, R10

	// Re-load partially accumulated output samples, wrapping ringbuffer
	MOVQ    R10, R11
	VMOVUPS (AX)(R11*4), Z0
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z1
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z2
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z3
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z4
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z5
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z6
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z7
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z8
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z9
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z10
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z11

	// For each input sample:
	XORQ R11, R11
	MOVQ In_len+32(FP), R12
	SUBQ $0x00000001, R12

In0:
	CMPQ R12, R11
	JL   In0end

	// The coefficient load index is (filtertaps+padding) * wrappedInputIndex
	// + (outAlignedIdx*vectorLength - outSampleIdx
	// This loads each coefficient set within a block with proper zero padding
	// so that multiple coefficient sets can be accumulated into one set of registers,
	// offset by the proper number of in-register samples
	// Broadcast the current input sample and contribute and accumulate its output-phase-specific-coefficient-scaled individual contribution to every output sample in range
	VBROADCASTSS (CX)(R11*4), Z12
	VFMADD231PS  (DX)(BX*4), Z12, Z0
	VFMADD231PS  64(DX)(BX*4), Z12, Z1
	VFMADD231PS  128(DX)(BX*4), Z12, Z2
	VFMADD231PS  192(DX)(BX*4), Z12, Z3
	VFMADD231PS  256(DX)(BX*4), Z12, Z4
	VFMADD231PS  320(DX)(BX*4), Z12, Z5
	VFMADD231PS  384(DX)(BX*4), Z12, Z6
	VFMADD231PS  448(DX)(BX*4), Z12, Z7
	VFMADD231PS  512(DX)(BX*4), Z12, Z8
	VFMADD231PS  576(DX)(BX*4), Z12, Z9
	VFMADD231PS  640(DX)(BX*4), Z12, Z10
	VFMADD231PS  704(DX)(BX*4), Z12, Z11

	// If incrementing the output index crosses a multiple of vectorLength,
	// the lowest register is completely accumulated and can be stored while
	// the rest are shifted down in its place
	// Compute current register index
	MOVQ DI, R13
	SHRQ $+36, R13

	// Increment current output sample index and compute next register index
	ADDQ R9, DI
	MOVQ DI, R14
	SHRQ $+36, R14

	// Store and shift registers if a register transition has happened
	CMPQ    R13, R14
	JE      no_store
	VMOVUPS Z0, (AX)(R10*4)
	VMOVUPS Z1, Z0
	VMOVUPS Z2, Z1
	VMOVUPS Z3, Z2
	VMOVUPS Z4, Z3
	VMOVUPS Z5, Z4
	VMOVUPS Z6, Z5
	VMOVUPS Z7, Z6
	VMOVUPS Z8, Z7
	VMOVUPS Z9, Z8
	VMOVUPS Z10, Z9
	VMOVUPS Z11, Z10
	VXORPS  Z11, Z11, Z11

no_store:
	// Update and wrap the vector-aligned output index
	MOVQ R14, R10
	SHLQ $+4, R10
	ANDQ R8, R10

	// Update and wrap coefficient index
	XORQ R13, R13
	ADDQ $+224, BX
	CMPQ BX, SI

	// Wrap phase counter - SUB changes flags so do this after to avoid clobbering Compare result
	CMOVQGE SI, R13
	SUBQ    R13, BX
	ADDQ    $0x00000001, R11
	JMP     In0

In0end:
	// Store each partially accumulated vector to the output slice
	// taking care to wrap into output ringbuffer
	VMOVUPS Z0, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z1, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z2, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z3, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z4, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z5, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z6, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z7, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z8, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z9, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z10, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z11, (AX)(R10*4)
	VZEROUPPER

	// Return the latest phase and output index for reuse in future calls
	MOVQ BX, CoefIdxOut+96(FP)
	MOVQ DI, OutIdxOut+104(FP)
	RET

// func ResampleFixedF32_16x13(Out []float32, In []float32, Coefs []float32, CoefIdx int, OutIdx int, OutStep int) (CoefIdxOut int, OutIdxOut int)
// Requires: AVX, AVX512DQ, AVX512F, CMOV
TEXT ·ResampleFixedF32_16x13(SB), NOSPLIT, $0-112
	MOVQ Out_base+0(FP), AX
	MOVQ In_base+24(FP), CX
	MOVQ Coefs_base+48(FP), DX
	MOVQ CoefIdx+72(FP), BX
	MOVQ Coefs_len+56(FP), SI
	MOVQ OutIdx+80(FP), DI
	MOVQ Out_len+8(FP), R8
	SUBQ $+1, R8
	MOVQ OutStep+88(FP), R9

	// Reload previous partially accumulated output samples
	// Compute output vector index as 
	// (fixedPointIndex / fixedPointScale / vectorLength * vectorLength) % outBufferLength
	// wraps within the output buffer and quantizes the nearest vector register multiple
	MOVQ DI, R10
	SHRQ $+36, R10
	SHLQ $+4, R10
	ANDQ R8, R10

	// Re-load partially accumulated output samples, wrapping ringbuffer
	MOVQ    R10, R11
	VMOVUPS (AX)(R11*4), Z0
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z1
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z2
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z3
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z4
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z5
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z6
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z7
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z8
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z9
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z10
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z11
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z12

	// For each input sample:
	XORQ R11, R11
	MOVQ In_len+32(FP), R12
	SUBQ $0x00000001, R12

In0:
	CMPQ R12, R11
	JL   In0end

	// The coefficient load index is (filtertaps+padding) * wrappedInputIndex
	// + (outAlignedIdx*vectorLength - outSampleIdx
	// This loads each coefficient set within a block with proper zero padding
	// so that multiple coefficient sets can be accumulated into one set of registers,
	// offset by the proper number of in-register samples
	// Broadcast the current input sample and contribute and accumulate its output-phase-specific-coefficient-scaled individual contribution to every output sample in range
	VBROADCASTSS (CX)(R11*4), Z13
	VFMADD231PS  (DX)(BX*4), Z13, Z0
	VFMADD231PS  64(DX)(BX*4), Z13, Z1
	VFMADD231PS  128(DX)(BX*4), Z13, Z2
	VFMADD231PS  192(DX)(BX*4), Z13, Z3
	VFMADD231PS  256(DX)(BX*4), Z13, Z4
	VFMADD231PS  320(DX)(BX*4), Z13, Z5
	VFMADD231PS  384(DX)(BX*4), Z13, Z6
	VFMADD231PS  448(DX)(BX*4), Z13, Z7
	VFMADD231PS  512(DX)(BX*4), Z13, Z8
	VFMADD231PS  576(DX)(BX*4), Z13, Z9
	VFMADD231PS  640(DX)(BX*4), Z13, Z10
	VFMADD231PS  704(DX)(BX*4), Z13, Z11
	VFMADD231PS  768(DX)(BX*4), Z13, Z12

	// If incrementing the output index crosses a multiple of vectorLength,
	// the lowest register is completely accumulated and can be stored while
	// the rest are shifted down in its place
	// Compute current register index
	MOVQ DI, R13
	SHRQ $+36, R13

	// Increment current output sample index and compute next register index
	ADDQ R9, DI
	MOVQ DI, R14
	SHRQ $+36, R14

	// Store and shift registers if a register transition has happened
	CMPQ    R13, R14
	JE      no_store
	VMOVUPS Z0, (AX)(R10*4)
	VMOVUPS Z1, Z0
	VMOVUPS Z2, Z1
	VMOVUPS Z3, Z2
	VMOVUPS Z4, Z3
	VMOVUPS Z5, Z4
	VMOVUPS Z6, Z5
	VMOVUPS Z7, Z6
	VMOVUPS Z8, Z7
	VMOVUPS Z9, Z8
	VMOVUPS Z10, Z9
	VMOVUPS Z11, Z10
	VMOVUPS Z12, Z11
	VXORPS  Z12, Z12, Z12

no_store:
	// Update and wrap the vector-aligned output index
	MOVQ R14, R10
	SHLQ $+4, R10
	ANDQ R8, R10

	// Update and wrap coefficient index
	XORQ R13, R13
	ADDQ $+240, BX
	CMPQ BX, SI

	// Wrap phase counter - SUB changes flags so do this after to avoid clobbering Compare result
	CMOVQGE SI, R13
	SUBQ    R13, BX
	ADDQ    $0x00000001, R11
	JMP     In0

In0end:
	// Store each partially accumulated vector to the output slice
	// taking care to wrap into output ringbuffer
	VMOVUPS Z0, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z1, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z2, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z3, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z4, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z5, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z6, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z7, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z8, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z9, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z10, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z11, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z12, (AX)(R10*4)
	VZEROUPPER

	// Return the latest phase and output index for reuse in future calls
	MOVQ BX, CoefIdxOut+96(FP)
	MOVQ DI, OutIdxOut+104(FP)
	RET

// func ResampleFixedF32_16x14(Out []float32, In []float32, Coefs []float32, CoefIdx int, OutIdx int, OutStep int) (CoefIdxOut int, OutIdxOut int)
// Requires: AVX, AVX512DQ, AVX512F, CMOV
TEXT ·ResampleFixedF32_16x14(SB), NOSPLIT, $0-112
	MOVQ Out_base+0(FP), AX
	MOVQ In_base+24(FP), CX
	MOVQ Coefs_base+48(FP), DX
	MOVQ CoefIdx+72(FP), BX
	MOVQ Coefs_len+56(FP), SI
	MOVQ OutIdx+80(FP), DI
	MOVQ Out_len+8(FP), R8
	SUBQ $+1, R8
	MOVQ OutStep+88(FP), R9

	// Reload previous partially accumulated output samples
	// Compute output vector index as 
	// (fixedPointIndex / fixedPointScale / vectorLength * vectorLength) % outBufferLength
	// wraps within the output buffer and quantizes the nearest vector register multiple
	MOVQ DI, R10
	SHRQ $+36, R10
	SHLQ $+4, R10
	ANDQ R8, R10

	// Re-load partially accumulated output samples, wrapping ringbuffer
	MOVQ    R10, R11
	VMOVUPS (AX)(R11*4), Z0
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z1
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z2
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z3
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z4
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z5
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z6
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z7
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z8
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z9
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z10
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z11
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z12
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z13

	// For each input sample:
	XORQ R11, R11
	MOVQ In_len+32(FP), R12
	SUBQ $0x00000001, R12

In0:
	CMPQ R12, R11
	JL   In0end

	// The coefficient load index is (filtertaps+padding) * wrappedInputIndex
	// + (outAlignedIdx*vectorLength - outSampleIdx
	// This loads each coefficient set within a block with proper zero padding
	// so that multiple coefficient sets can be accumulated into one set of registers,
	// offset by the proper number of in-register samples
	// Broadcast the current input sample and contribute and accumulate its output-phase-specific-coefficient-scaled individual contribution to every output sample in range
	VBROADCASTSS (CX)(R11*4), Z14
	VFMADD231PS  (DX)(BX*4), Z14, Z0
	VFMADD231PS  64(DX)(BX*4), Z14, Z1
	VFMADD231PS  128(DX)(BX*4), Z14, Z2
	VFMADD231PS  192(DX)(BX*4), Z14, Z3
	VFMADD231PS  256(DX)(BX*4), Z14, Z4
	VFMADD231PS  320(DX)(BX*4), Z14, Z5
	VFMADD231PS  384(DX)(BX*4), Z14, Z6
	VFMADD231PS  448(DX)(BX*4), Z14, Z7
	VFMADD231PS  512(DX)(BX*4), Z14, Z8
	VFMADD231PS  576(DX)(BX*4), Z14, Z9
	VFMADD231PS  640(DX)(BX*4), Z14, Z10
	VFMADD231PS  704(DX)(BX*4), Z14, Z11
	VFMADD231PS  768(DX)(BX*4), Z14, Z12
	VFMADD231PS  832(DX)(BX*4), Z14, Z13

	// If incrementing the output index crosses a multiple of vectorLength,
	// the lowest register is completely accumulated and can be stored while
	// the rest are shifted down in its place
	// Compute current register index
	MOVQ DI, R13
	SHRQ $+36, R13

	// Increment current output sample index and compute next register index
	ADDQ R9, DI
	MOVQ DI, R14
	SHRQ $+36, R14

	// Store and shift registers if a register transition has happened
	CMPQ    R13, R14
	JE      no_store
	VMOVUPS Z0, (AX)(R10*4)
	VMOVUPS Z1, Z0
	VMOVUPS Z2, Z1
	VMOVUPS Z3, Z2
	VMOVUPS Z4, Z3
	VMOVUPS Z5, Z4
	VMOVUPS Z6, Z5
	VMOVUPS Z7, Z6
	VMOVUPS Z8, Z7
	VMOVUPS Z9, Z8
	VMOVUPS Z10, Z9
	VMOVUPS Z11, Z10
	VMOVUPS Z12, Z11
	VMOVUPS Z13, Z12
	VXORPS  Z13, Z13, Z13

no_store:
	// Update and wrap the vector-aligned output index
	MOVQ R14, R10
	SHLQ $+4, R10
	ANDQ R8, R10

	// Update and wrap coefficient index
	XORQ R13, R13
	ADDQ $+256, BX
	CMPQ BX, SI

	// Wrap phase counter - SUB changes flags so do this after to avoid clobbering Compare result
	CMOVQGE SI, R13
	SUBQ    R13, BX
	ADDQ    $0x00000001, R11
	JMP     In0

In0end:
	// Store each partially accumulated vector to the output slice
	// taking care to wrap into output ringbuffer
	VMOVUPS Z0, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z1, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z2, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z3, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z4, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z5, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z6, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z7, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z8, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z9, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z10, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z11, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z12, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z13, (AX)(R10*4)
	VZEROUPPER

	// Return the latest phase and output index for reuse in future calls
	MOVQ BX, CoefIdxOut+96(FP)
	MOVQ DI, OutIdxOut+104(FP)
	RET

// func ResampleFixedF32_16x15(Out []float32, In []float32, Coefs []float32, CoefIdx int, OutIdx int, OutStep int) (CoefIdxOut int, OutIdxOut int)
// Requires: AVX, AVX512DQ, AVX512F, CMOV
TEXT ·ResampleFixedF32_16x15(SB), NOSPLIT, $0-112
	MOVQ Out_base+0(FP), AX
	MOVQ In_base+24(FP), CX
	MOVQ Coefs_base+48(FP), DX
	MOVQ CoefIdx+72(FP), BX
	MOVQ Coefs_len+56(FP), SI
	MOVQ OutIdx+80(FP), DI
	MOVQ Out_len+8(FP), R8
	SUBQ $+1, R8
	MOVQ OutStep+88(FP), R9

	// Reload previous partially accumulated output samples
	// Compute output vector index as 
	// (fixedPointIndex / fixedPointScale / vectorLength * vectorLength) % outBufferLength
	// wraps within the output buffer and quantizes the nearest vector register multiple
	MOVQ DI, R10
	SHRQ $+36, R10
	SHLQ $+4, R10
	ANDQ R8, R10

	// Re-load partially accumulated output samples, wrapping ringbuffer
	MOVQ    R10, R11
	VMOVUPS (AX)(R11*4), Z0
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z1
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z2
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z3
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z4
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z5
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z6
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z7
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z8
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z9
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z10
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z11
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z12
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z13
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z14

	// For each input sample:
	XORQ R11, R11
	MOVQ In_len+32(FP), R12
	SUBQ $0x00000001, R12

In0:
	CMPQ R12, R11
	JL   In0end

	// The coefficient load index is (filtertaps+padding) * wrappedInputIndex
	// + (outAlignedIdx*vectorLength - outSampleIdx
	// This loads each coefficient set within a block with proper zero padding
	// so that multiple coefficient sets can be accumulated into one set of registers,
	// offset by the proper number of in-register samples
	// Broadcast the current input sample and contribute and accumulate its output-phase-specific-coefficient-scaled individual contribution to every output sample in range
	VBROADCASTSS (CX)(R11*4), Z15
	VFMADD231PS  (DX)(BX*4), Z15, Z0
	VFMADD231PS  64(DX)(BX*4), Z15, Z1
	VFMADD231PS  128(DX)(BX*4), Z15, Z2
	VFMADD231PS  192(DX)(BX*4), Z15, Z3
	VFMADD231PS  256(DX)(BX*4), Z15, Z4
	VFMADD231PS  320(DX)(BX*4), Z15, Z5
	VFMADD231PS  384(DX)(BX*4), Z15, Z6
	VFMADD231PS  448(DX)(BX*4), Z15, Z7
	VFMADD231PS  512(DX)(BX*4), Z15, Z8
	VFMADD231PS  576(DX)(BX*4), Z15, Z9
	VFMADD231PS  640(DX)(BX*4), Z15, Z10
	VFMADD231PS  704(DX)(BX*4), Z15, Z11
	VFMADD231PS  768(DX)(BX*4), Z15, Z12
	VFMADD231PS  832(DX)(BX*4), Z15, Z13
	VFMADD231PS  896(DX)(BX*4), Z15, Z14

	// If incrementing the output index crosses a multiple of vectorLength,
	// the lowest register is completely accumulated and can be stored while
	// the rest are shifted down in its place
	// Compute current register index
	MOVQ DI, R13
	SHRQ $+36, R13

	// Increment current output sample index and compute next register index
	ADDQ R9, DI
	MOVQ DI, R14
	SHRQ $+36, R14

	// Store and shift registers if a register transition has happened
	CMPQ    R13, R14
	JE      no_store
	VMOVUPS Z0, (AX)(R10*4)
	VMOVUPS Z1, Z0
	VMOVUPS Z2, Z1
	VMOVUPS Z3, Z2
	VMOVUPS Z4, Z3
	VMOVUPS Z5, Z4
	VMOVUPS Z6, Z5
	VMOVUPS Z7, Z6
	VMOVUPS Z8, Z7
	VMOVUPS Z9, Z8
	VMOVUPS Z10, Z9
	VMOVUPS Z11, Z10
	VMOVUPS Z12, Z11
	VMOVUPS Z13, Z12
	VMOVUPS Z14, Z13
	VXORPS  Z14, Z14, Z14

no_store:
	// Update and wrap the vector-aligned output index
	MOVQ R14, R10
	SHLQ $+4, R10
	ANDQ R8, R10

	// Update and wrap coefficient index
	XORQ R13, R13
	ADDQ $+272, BX
	CMPQ BX, SI

	// Wrap phase counter - SUB changes flags so do this after to avoid clobbering Compare result
	CMOVQGE SI, R13
	SUBQ    R13, BX
	ADDQ    $0x00000001, R11
	JMP     In0

In0end:
	// Store each partially accumulated vector to the output slice
	// taking care to wrap into output ringbuffer
	VMOVUPS Z0, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z1, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z2, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z3, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z4, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z5, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z6, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z7, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z8, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z9, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z10, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z11, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z12, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z13, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z14, (AX)(R10*4)
	VZEROUPPER

	// Return the latest phase and output index for reuse in future calls
	MOVQ BX, CoefIdxOut+96(FP)
	MOVQ DI, OutIdxOut+104(FP)
	RET

// func ResampleFixedF32_16x16(Out []float32, In []float32, Coefs []float32, CoefIdx int, OutIdx int, OutStep int) (CoefIdxOut int, OutIdxOut int)
// Requires: AVX, AVX512DQ, AVX512F, CMOV
TEXT ·ResampleFixedF32_16x16(SB), NOSPLIT, $0-112
	MOVQ Out_base+0(FP), AX
	MOVQ In_base+24(FP), CX
	MOVQ Coefs_base+48(FP), DX
	MOVQ CoefIdx+72(FP), BX
	MOVQ Coefs_len+56(FP), SI
	MOVQ OutIdx+80(FP), DI
	MOVQ Out_len+8(FP), R8
	SUBQ $+1, R8
	MOVQ OutStep+88(FP), R9

	// Reload previous partially accumulated output samples
	// Compute output vector index as 
	// (fixedPointIndex / fixedPointScale / vectorLength * vectorLength) % outBufferLength
	// wraps within the output buffer and quantizes the nearest vector register multiple
	MOVQ DI, R10
	SHRQ $+36, R10
	SHLQ $+4, R10
	ANDQ R8, R10

	// Re-load partially accumulated output samples, wrapping ringbuffer
	MOVQ    R10, R11
	VMOVUPS (AX)(R11*4), Z0
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z1
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z2
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z3
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z4
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z5
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z6
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z7
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z8
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z9
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z10
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z11
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z12
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z13
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z14
	ADDQ    $+16, R11
	ANDQ    R8, R11
	VMOVUPS (AX)(R11*4), Z15

	// For each input sample:
	XORQ R11, R11
	MOVQ In_len+32(FP), R12
	SUBQ $0x00000001, R12

In0:
	CMPQ R12, R11
	JL   In0end

	// The coefficient load index is (filtertaps+padding) * wrappedInputIndex
	// + (outAlignedIdx*vectorLength - outSampleIdx
	// This loads each coefficient set within a block with proper zero padding
	// so that multiple coefficient sets can be accumulated into one set of registers,
	// offset by the proper number of in-register samples
	// Broadcast the current input sample and contribute and accumulate its output-phase-specific-coefficient-scaled individual contribution to every output sample in range
	VBROADCASTSS (CX)(R11*4), Z16
	VFMADD231PS  (DX)(BX*4), Z16, Z0
	VFMADD231PS  64(DX)(BX*4), Z16, Z1
	VFMADD231PS  128(DX)(BX*4), Z16, Z2
	VFMADD231PS  192(DX)(BX*4), Z16, Z3
	VFMADD231PS  256(DX)(BX*4), Z16, Z4
	VFMADD231PS  320(DX)(BX*4), Z16, Z5
	VFMADD231PS  384(DX)(BX*4), Z16, Z6
	VFMADD231PS  448(DX)(BX*4), Z16, Z7
	VFMADD231PS  512(DX)(BX*4), Z16, Z8
	VFMADD231PS  576(DX)(BX*4), Z16, Z9
	VFMADD231PS  640(DX)(BX*4), Z16, Z10
	VFMADD231PS  704(DX)(BX*4), Z16, Z11
	VFMADD231PS  768(DX)(BX*4), Z16, Z12
	VFMADD231PS  832(DX)(BX*4), Z16, Z13
	VFMADD231PS  896(DX)(BX*4), Z16, Z14
	VFMADD231PS  960(DX)(BX*4), Z16, Z15

	// If incrementing the output index crosses a multiple of vectorLength,
	// the lowest register is completely accumulated and can be stored while
	// the rest are shifted down in its place
	// Compute current register index
	MOVQ DI, R13
	SHRQ $+36, R13

	// Increment current output sample index and compute next register index
	ADDQ R9, DI
	MOVQ DI, R14
	SHRQ $+36, R14

	// Store and shift registers if a register transition has happened
	CMPQ    R13, R14
	JE      no_store
	VMOVUPS Z0, (AX)(R10*4)
	VMOVUPS Z1, Z0
	VMOVUPS Z2, Z1
	VMOVUPS Z3, Z2
	VMOVUPS Z4, Z3
	VMOVUPS Z5, Z4
	VMOVUPS Z6, Z5
	VMOVUPS Z7, Z6
	VMOVUPS Z8, Z7
	VMOVUPS Z9, Z8
	VMOVUPS Z10, Z9
	VMOVUPS Z11, Z10
	VMOVUPS Z12, Z11
	VMOVUPS Z13, Z12
	VMOVUPS Z14, Z13
	VMOVUPS Z15, Z14
	VXORPS  Z15, Z15, Z15

no_store:
	// Update and wrap the vector-aligned output index
	MOVQ R14, R10
	SHLQ $+4, R10
	ANDQ R8, R10

	// Update and wrap coefficient index
	XORQ R13, R13
	ADDQ $+288, BX
	CMPQ BX, SI

	// Wrap phase counter - SUB changes flags so do this after to avoid clobbering Compare result
	CMOVQGE SI, R13
	SUBQ    R13, BX
	ADDQ    $0x00000001, R11
	JMP     In0

In0end:
	// Store each partially accumulated vector to the output slice
	// taking care to wrap into output ringbuffer
	VMOVUPS Z0, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z1, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z2, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z3, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z4, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z5, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z6, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z7, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z8, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z9, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z10, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z11, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z12, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z13, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z14, (AX)(R10*4)
	ADDQ    $+16, R10
	ANDQ    R8, R10
	VMOVUPS Z15, (AX)(R10*4)
	VZEROUPPER

	// Return the latest phase and output index for reuse in future calls
	MOVQ BX, CoefIdxOut+96(FP)
	MOVQ DI, OutIdxOut+104(FP)
	RET

// func ResampleF32x64_8x8(Out []float32, In []float32, Coefs []float32, PhaseIdx int, Phases int, SubsampleIdx uint64, SubsampleDelta uint64, Taps int) (PhaseIdxOut int, SubsampleIdxOut uint64)
// Requires: AVX, CMOV, FMA3
TEXT ·ResampleF32x64_8x8(SB), NOSPLIT, $0-128
	MOVQ Out_base+0(FP), AX
	MOVQ In_base+24(FP), CX
	MOVQ Coefs_base+48(FP), DX
	MOVQ PhaseIdx+72(FP), BX
	MOVQ Phases+80(FP), SI
	MOVQ SubsampleIdx+88(FP), DI
	MOVQ SubsampleDelta+96(FP), R8
	MOVQ Taps+104(FP), R9

	// Reload previous partially accumulated output samples
	VMOVUPS (AX), Y0
	VMOVUPS 32(AX), Y1
	VMOVUPS 64(AX), Y2
	VMOVUPS 96(AX), Y3
	VMOVUPS 128(AX), Y4
	VMOVUPS 160(AX), Y5
	VMOVUPS 192(AX), Y6
	VMOVUPS 224(AX), Y7
	XORQ    R10, R10
	XORQ    R11, R11
	MOVQ    In_len+32(FP), R12
	SUBQ    $0x00000001, R12

In0:
	CMPQ R12, R11
	JL   In0end

	// The coefficient load index is filtertaps * phase index
	MOVQ  BX, R13
	IMULQ R9, R13

	// Broadcast the current input sample and contribute and accumulate its output-phase-specific-coefficient-scaled individual contribution to every output sample in range
	VBROADCASTSS (CX)(R11*4), Y8
	VFMADD231PS  (DX)(R13*4), Y8, Y0
	VFMADD231PS  32(DX)(R13*4), Y8, Y1
	VFMADD231PS  64(DX)(R13*4), Y8, Y2
	VFMADD231PS  96(DX)(R13*4), Y8, Y3
	VFMADD231PS  128(DX)(R13*4), Y8, Y4
	VFMADD231PS  160(DX)(R13*4), Y8, Y5
	VFMADD231PS  192(DX)(R13*4), Y8, Y6
	VFMADD231PS  224(DX)(R13*4), Y8, Y7

	// If adding maxUint/resampleRatio/simdLen overflows the subsample counter, advance to the next output block
	// The lowest block is completed and further input samples have no contribution
	ADDQ    R8, DI
	JNO     no_store
	VMOVUPS Y0, (AX)(R10*4)
	VMOVUPS Y1, Y0
	VMOVUPS Y2, Y1
	VMOVUPS Y3, Y2
	VMOVUPS Y4, Y3
	VMOVUPS Y5, Y4
	VMOVUPS Y6, Y5
	VMOVUPS Y7, Y6
	VXORPS  Y7, Y7, Y7
	ADDQ    $+8, R10

no_store:
	// Update and wrap phase index counter
	XORQ R13, R13
	ADDQ $+1, BX
	CMPQ BX, SI

	// If phase index counter was wrapped, reset subsample counter to one
	MOVQ    $0x0000000000000001, R14
	CMOVQGE R14, DI

	// Wrap phase counter - SUB changes flags so do this after to avoid clobbering Compare result
	CMOVQGE SI, R13
	SUBQ    R13, BX
	ADDQ    $0x00000001, R11
	JMP     In0

In0end:
	// Save partial outputs
	VMOVUPS Y0, (AX)(R10*4)
	VMOVUPS Y1, 32(AX)(R10*4)
	VMOVUPS Y2, 64(AX)(R10*4)
	VMOVUPS Y3, 96(AX)(R10*4)
	VMOVUPS Y4, 128(AX)(R10*4)
	VMOVUPS Y5, 160(AX)(R10*4)
	VMOVUPS Y6, 192(AX)(R10*4)
	VMOVUPS Y7, 224(AX)(R10*4)
	VZEROUPPER

	// Return the latest phase index for reuse in future calls
	MOVQ BX, PhaseIdxOut+112(FP)
	MOVQ DI, SubsampleIdxOut+120(FP)
	RET

// func ResampleF32x64_16x16(Out []float32, In []float32, Coefs []float32, PhaseIdx int, Phases int, SubsampleIdx uint64, SubsampleDelta uint64, Taps int) (PhaseIdxOut int, SubsampleIdxOut uint64)
// Requires: AVX, AVX512DQ, AVX512F, CMOV
TEXT ·ResampleF32x64_16x16(SB), NOSPLIT, $0-128
	MOVQ Out_base+0(FP), AX
	MOVQ In_base+24(FP), CX
	MOVQ Coefs_base+48(FP), DX
	MOVQ PhaseIdx+72(FP), BX
	MOVQ Phases+80(FP), SI
	MOVQ SubsampleIdx+88(FP), DI
	MOVQ SubsampleDelta+96(FP), R8
	MOVQ Taps+104(FP), R9

	// Reload previous partially accumulated output samples
	VMOVUPS (AX), Z0
	VMOVUPS 64(AX), Z1
	VMOVUPS 128(AX), Z2
	VMOVUPS 192(AX), Z3
	VMOVUPS 256(AX), Z4
	VMOVUPS 320(AX), Z5
	VMOVUPS 384(AX), Z6
	VMOVUPS 448(AX), Z7
	VMOVUPS 512(AX), Z8
	VMOVUPS 576(AX), Z9
	VMOVUPS 640(AX), Z10
	VMOVUPS 704(AX), Z11
	VMOVUPS 768(AX), Z12
	VMOVUPS 832(AX), Z13
	VMOVUPS 896(AX), Z14
	VMOVUPS 960(AX), Z15
	XORQ    R10, R10
	XORQ    R11, R11
	MOVQ    In_len+32(FP), R12
	SUBQ    $0x00000001, R12

In0:
	CMPQ R12, R11
	JL   In0end

	// The coefficient load index is filtertaps * phase index
	MOVQ  BX, R13
	IMULQ R9, R13

	// Broadcast the current input sample and contribute and accumulate its output-phase-specific-coefficient-scaled individual contribution to every output sample in range
	VBROADCASTSS (CX)(R11*4), Z16
	VFMADD231PS  (DX)(R13*4), Z16, Z0
	VFMADD231PS  64(DX)(R13*4), Z16, Z1
	VFMADD231PS  128(DX)(R13*4), Z16, Z2
	VFMADD231PS  192(DX)(R13*4), Z16, Z3
	VFMADD231PS  256(DX)(R13*4), Z16, Z4
	VFMADD231PS  320(DX)(R13*4), Z16, Z5
	VFMADD231PS  384(DX)(R13*4), Z16, Z6
	VFMADD231PS  448(DX)(R13*4), Z16, Z7
	VFMADD231PS  512(DX)(R13*4), Z16, Z8
	VFMADD231PS  576(DX)(R13*4), Z16, Z9
	VFMADD231PS  640(DX)(R13*4), Z16, Z10
	VFMADD231PS  704(DX)(R13*4), Z16, Z11
	VFMADD231PS  768(DX)(R13*4), Z16, Z12
	VFMADD231PS  832(DX)(R13*4), Z16, Z13
	VFMADD231PS  896(DX)(R13*4), Z16, Z14
	VFMADD231PS  960(DX)(R13*4), Z16, Z15

	// If adding maxUint/resampleRatio/simdLen overflows the subsample counter, advance to the next output block
	// The lowest block is completed and further input samples have no contribution
	ADDQ    R8, DI
	JNO     no_store
	VMOVUPS Z0, (AX)(R10*4)
	VMOVUPS Z1, Z0
	VMOVUPS Z2, Z1
	VMOVUPS Z3, Z2
	VMOVUPS Z4, Z3
	VMOVUPS Z5, Z4
	VMOVUPS Z6, Z5
	VMOVUPS Z7, Z6
	VMOVUPS Z8, Z7
	VMOVUPS Z9, Z8
	VMOVUPS Z10, Z9
	VMOVUPS Z11, Z10
	VMOVUPS Z12, Z11
	VMOVUPS Z13, Z12
	VMOVUPS Z14, Z13
	VMOVUPS Z15, Z14
	VXORPS  Z15, Z15, Z15
	ADDQ    $+16, R10

no_store:
	// Update and wrap phase index counter
	XORQ R13, R13
	ADDQ $+1, BX
	CMPQ BX, SI

	// If phase index counter was wrapped, reset subsample counter to one
	MOVQ    $0x0000000000000001, R14
	CMOVQGE R14, DI

	// Wrap phase counter - SUB changes flags so do this after to avoid clobbering Compare result
	CMOVQGE SI, R13
	SUBQ    R13, BX
	ADDQ    $0x00000001, R11
	JMP     In0

In0end:
	// Save partial outputs
	VMOVUPS Z0, (AX)(R10*4)
	VMOVUPS Z1, 64(AX)(R10*4)
	VMOVUPS Z2, 128(AX)(R10*4)
	VMOVUPS Z3, 192(AX)(R10*4)
	VMOVUPS Z4, 256(AX)(R10*4)
	VMOVUPS Z5, 320(AX)(R10*4)
	VMOVUPS Z6, 384(AX)(R10*4)
	VMOVUPS Z7, 448(AX)(R10*4)
	VMOVUPS Z8, 512(AX)(R10*4)
	VMOVUPS Z9, 576(AX)(R10*4)
	VMOVUPS Z10, 640(AX)(R10*4)
	VMOVUPS Z11, 704(AX)(R10*4)
	VMOVUPS Z12, 768(AX)(R10*4)
	VMOVUPS Z13, 832(AX)(R10*4)
	VMOVUPS Z14, 896(AX)(R10*4)
	VMOVUPS Z15, 960(AX)(R10*4)
	VZEROUPPER

	// Return the latest phase index for reuse in future calls
	MOVQ BX, PhaseIdxOut+112(FP)
	MOVQ DI, SubsampleIdxOut+120(FP)
	RET

// func SincF32s_8x1(Out []float32, Xmin float32, Xmax float32)
// Requires: AVX
TEXT ·SincF32s_8x1(SB), NOSPLIT, $0-32
	VZEROUPPER
	RET
